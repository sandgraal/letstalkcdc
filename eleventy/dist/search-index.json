[
  {
    "path": "/",
    "title": "/",
    "text": "Learn why Change Data Capture (CDC) projects fail and how to build scalable, reliable, and production-ready data pipelines. A deep-dive series. Why Change Data Capture Still Breaks, and How To Get It Right. In the modern data landscape , the value of information is intrinsically linked to its timeliness. Decisions made on outdated data can lead to missed opportunities, inefficient operations, and a diminished competitive edge. This reality has exposed the limitations of traditional data integration methods like nightly batch jobs and paved the way for a more dynamic, real-time approach. This series shows you how to build production-grade CDC pipelines that deliver on the promise of real-time data. What is Change Data Capture (CDC)? Change Data Capture (CDC) is a set of software design patterns used to identify, capture, and deliver the changes made to data in a source system, most commonly a database. Instead of copying entire datasets at intervals, CDC focuses exclusively on incremental, row-level changes like INSERT, UPDATE, and DELETE operations. Once captured, these change events are delivered in real-time or near-real-time to downstream systems such as data warehouses, analyti…"
  },
  {
    "path": "/overview/",
    "title": "Series Overview",
    "text": "Series hub for advanced CDC patterns. Exactly-once semantics, multi-tenancy, partitioning, and reconciliation. Series Overview Change Data Capture marks a fundamental evolution in data integration, moving away from the latent, resource-intensive world of batch processing and into the dynamic paradigm of real-time streaming. By capturing individual data changes as they occur, CDC provides a mechanism to keep disparate systems synchronized with minimal impact and sub-second latency. This technology is a strategic enabler, unlocking real-time analytics and forming the backbone of resilient, modern data architectures. Start with the fundamentals. Understand what Change Data Capture is and why it's a cornerstone of modern data architecture through real-world use cases. Start Here Core Concept Interactive Introduction to CDC An interactive dashboard covering core concepts, methods, architectures, and the tooling ecosystem. Dive In! Core Concept Event Envelope Delivery Guarantees Keys vs payload, before/after images, tombstones; ALO vs EOS scope and per-key ordering. Coming Soon Core Concept Materialization 101 (Upsert/Delete) Practical MERGE patterns for upserts deletes; compaction vs hi…"
  },
  {
    "path": "/intro/",
    "title": "Interactive Introduction to CDC",
    "text": "An interactive dashboard covering core CDC concepts, methods, architectures, and the tooling ecosystem. Interactive Introduction to CDC Change Data Capture ( CDC ) is the discipline of replicating data changes from a source database to downstream systems in near real time—without heavy full refreshes. For decades, moving data between systems meant relying on slow, resource-intensive batch jobs that would run overnight. This approach is no longer viable in a world that demands real-time data. How do you keep disparate systems synchronized instantly without overwhelming your databases? This is the core problem that Change Data Capture (CDC) solves. CDC methods Log-based CDC (The Gold Standard): Reads committed changes from the database’s transaction log ( PostgreSQL WAL , MySQL binlog , SQL Server transaction log , Oracle redo ). It’s low-impact on OLTP, but not free: log retention, I/O, and replication slot/archival settings must be tuned. It reliably captures inserts/updates/deletes in order. DDL capture depends on the database and connector configuration. PostgreSQL's Write-Ahead Log (WAL) records all changes to the database before they are applied, ensuring data integrity and dur…"
  },
  {
    "path": "/exactly-once/",
    "title": "Exactly-Once Semantics",
    "text": "A deep dive into at-least-once vs. exactly-once semantics, idempotency, and the transactional outbox pattern for reliable data pipelines. The Pragmatic Reality of Exactly-Once Processing End-to-end \"exactly-once delivery \" across heterogeneous systems (DB → Kafka → warehouse/search) is not achievable in general. What we actually ship is Exactly-Once Processing (EOP) : combine At-Least-Once (ALO) transport with idempotent sinks so replays don’t create duplicates. Scope matters: Source connectors (e.g., Debezium): ALO into Kafka. They can’t make the external database “transactional” with Kafka. Within Kafka→Kafka topologies: EOS is possible using Kafka transactions (idempotent producers + transactional offsets), but the scope is limited to Kafka topics. Kafka → external sinks: Effective EOS comes from idempotent upserts (or staging+MERGE) at the destination, not from transport guarantees alone. How a CDC pipeline behaves during failures is defined by its data delivery guarantees: At-Most-Once: The weakest guarantee. A message is sent once, but if a failure occurs before it's processed, the message is lost forever. Unsuitable for critical data. At-Least-Once: The most common default. …"
  },
  {
    "path": "/snapshotting/",
    "title": "Snapshotting (Initial Load)",
    "text": "Learn how to handle the initial data load (snapshotting) in a CDC pipeline consistently and without data loss. The First Hurdle: Initial Data Load (Snapshotting) What you’ll learn Why snapshots need a log high-watermark and idempotent sinks. How to choose snapshot modes (initial vs incremental). How to size chunks and set parallelism safely. How to run a 15-minute lab to prove no gaps/dupes. What to monitor and how to recover failures. Pre-flight checklist Retention window covers full snapshot duration (WAL/binlog/T-log/UNDO). Privileges for log reading/replication are granted. Consistent-read isolation is enabled where applicable ( REPEATABLE READ/SNAPSHOT). Sink MERGE/UPSERT implemented; delete/tombstone semantics defined. Offsets are durable and backed up/versioned. DDL freeze or schema-compatibility policy in place. Per-DB: boundary + consistent read (snippets) PostgreSQL Copy -- Requirements (once/admin): ALTER SYSTEM SET wal_level = logical; ALTER SYSTEM SET max_replication_slots = 10; -- adjust SELECT pg_reload_conf(); -- Boundary + consistent read (within one tx while snapshotting): BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ; SELECT pg_current_wal_lsn(); -- record th…"
  },
  {
    "path": "/multi-tenancy/",
    "title": "Multi-Tenancy — Isolation vs Cost | CDC: The Missing Manual",
    "text": "Interactive explorer for multi-tenant CDC: isolation levels, topic strategy, consumer groups, and egress sizing with shareable presets. Multi-Tenancy: Cost vs Isolation Primer: Why multi-tenancy matters for CDC Sharing infrastructure saves money but couples tenants together. More isolation (per-tenant topics or clusters) reduces blast radius and gives clearer SLAs, but increases cost and operational load. Isolation ladder: shared topics → per-tenant topics → per-tenant clusters. Egress (external) ≈ tenants × change_rate × (payload + envelope + overhead) . Broker egress adds replication factor: intra_broker ≈ external × (RF − 1) . Storage at retention ≈ egress_bytes_per_s × (86400 × days) ÷ compression . Operational heat: total partitions = topics × partitions_per_topic (impacts rebalances, ISR, & quotas). RBAC/retention easier per-tenant topics; cluster isolation for regulators/VIPs. Guardrails: per-tenant produce/consume quotas, DLQ policy, retention/compaction per topic, and ACLs scoped by tenant namespace. Rule-of-thumb cutovers ↑ tenants low compliance ⇒ shared topics until ops pain shows. ↑ compliance / RBAC / per-tenant SLAs ⇒ per-tenant topics. Regulatory isolation / VIPs / …"
  },
  {
    "path": "/partitioning/",
    "title": "Partitioning & Reconciliation — Keys, Skew, Late Arrivals | CDC: The Missing Manual",
    "text": "Learn partitioning and reconciliation for CDC: choosing keys, handling skew, preserving per-key order, and making late arrivals safe with version and watermarks. Partitioning & Reconciliation The Golden Rule of Kafka Ordering Kafka guarantees the order of messages within a single partition . It makes no ordering guarantees across different partitions. Therefore, to maintain the correct sequence of changes for any given entity (like a specific customer or a single product), all changes for that entity must be sent to the same partition. This is the fundamental purpose of a partition key. Partition sizing (rule-of-thumb) Start with a target throughput per partition your consumers can sustain (1–5k events/s). Baseline: partitions ≈ ceil(total_events_per_s ÷ target_per_partition) Include skew headroom: multiply by 1.5–2× if distribution is Zipf-y or you have hot tenants. Changing partitions later: adding partitions does not rehash existing keys; plan key strategy up front or use topic splits. How to Choose a Good Partition Key The quality of your partition key directly impacts both the correctness and performance of your system. Good Keys (Preserve Order & Distribute Load) customer_id …"
  },
  {
    "path": "/schema-evolution/",
    "title": "Schema Evolution | CDC: The Missing Manual",
    "text": "The Inevitable Problem: Handling Schema Evolution Source database schemas are not static. Over the lifecycle of an application, developers will inevitably make changes: adding new columns, removing old ones, or modifying data types. This phenomenon, known as schema drift or schema evolution , is a primary cause of brittle data pipelines. A pipeline not designed for this will break the moment an incoming change event no longer matches what downstream consumers expect. The Solution: The Schema Registry (An API Contract for Your Data) The industry-standard solution is to treat your data's schema like a formal API contract. This is managed by a Schema Registry . Think of it as the 'OpenAPI' or 'Swagger' for your event streams. It's a centralized service that ensures all data producers and consumers agree on the 'shape' of the data, even as that shape evolves over time. By using a schema-aware format like Apache Avro or Protobuf, producers and consumers can be decoupled, allowing them to evolve independently without breaking the pipeline. Choosing Your Contract Strategy: Compatibility Modes A schema registry isn't just a database of schemas; it actively enforces rules. When a producer t…"
  },
  {
    "path": "/strategy/",
    "title": "The Strategic Value of CDC | CDC: The Missing Manual",
    "text": "The Strategic Advantage: Embracing an Event-Driven Future Change Data Capture is more than a technical pattern; it is a strategic enabler that translates directly into tangible business value. By shifting from periodic batch updates to a continuous stream of data changes, organizations can enhance decision-making, improve operational efficiency, and power modern digital initiatives. Business case: where ROI shows up Revenue lift: fresher recommendations, faster quoting, real-time inventory → higher conversion/attach. Cost reduction: retire nightly ETL, incremental loads instead of full rebuilds, fewer bespoke APIs. Risk & compliance: auditability of changes, faster fraud/abuse detection, consistent deletes via tombstones. Time-to-market: teams subscribe to streams without negotiating point-to-point integrations. A Philosophical Shift for Data Traditional data integration asks, \"What is the current state of the data?\" This state-oriented view leads to latent, point-in-time snapshots. CDC fundamentally changes the question to, \"What just happened to the data?\" Instead of treating a database as a passive repository to be queried periodically, CDC transforms it into an active, real-tim…"
  },
  {
    "path": "/use-cases/",
    "title": "CDC Use Cases & Applications | CDC: The Missing Manual",
    "text": "CDC in the Wild: Real-World Applications Beyond the theory, Change Data Capture enables a diverse set of powerful, real-time applications. Understanding these patterns is key to unlocking the strategic value of your data. Each use case demonstrates a shift from slow, periodic batch processing to a continuous, event-driven paradigm. Use Case 1: Analytics & Business Intelligence From Batch ETL to Real-Time ELT The Problem: Business decisions based on stale, 24-hour-old data are a competitive liability. Traditional nightly ETL jobs place a heavy, recurring load on production databases and deliver insights that are already out of date. The CDC Solution: Log-based CDC continuously streams every row-level change from operational (OLTP) databases. These events flow through a streaming platform like Kafka and are loaded into an analytical data warehouse (Snowflake, BigQuery, Redshift) in near real-time. The warehouse uses `MERGE` or `UPSERT` operations to efficiently apply these granular changes, keeping analytical tables perfectly synchronized with the source. Producer (OLTP/CDC) Streaming Platform Warehouse CDC tails DB changes Publish to Kafka topic(s) Connector reads — — MERGE/UPSERT i…"
  },
  {
    "path": "/tooling/",
    "title": "CDC Tooling Comparison | CDC: The Missing Manual",
    "text": "The Modern CDC Toolkit: A Platform Comparison The market for Change Data Capture tools has matured significantly, offering a range of solutions that cater to different needs. Choosing the right tool involves balancing control, cost, and convenience. Open-Source Champions: Power and Flexibility Open-source tools are favored by organizations with strong in-house data engineering capabilities that require deep customization and want to avoid vendor lock-in. Debezium Debezium has emerged as the de facto open-source standard for log-based CDC. It is a distributed platform of connectors that runs on the Apache Kafka Connect framework, providing high-performance, low-latency connectors for a wide range of popular databases. Pros: Free to use (Apache 2.0 license), highly flexible, robust feature set, and backed by a large community. Cons: The primary challenge is operational complexity. It requires users to deploy, manage, scale, and monitor the underlying infrastructure, including Kafka and Kafka Connect, which requires significant technical expertise. Airbyte (OSS core + Cloud) Airbyte provides a large connector catalog (including CDC connectors) with an OSS core and a hosted “Cloud” off…"
  },
  {
    "path": "/merge-cookbook/",
    "title": "merge / upsert cookbook — sinks for CDC",
    "text": "shared assumptions (adapt to your schema) staging table: STG_CUSTOMERS (raw CDC) with columns ID , EMAIL , OP (c/u/d), OP_TS (event time), optional VERSION or LSN/GTID/SCN . target table: TARGET_CUSTOMERS (latest state). business key: ID . replace with your key columns as needed. late events: we accept an incoming row if its OP_TS ≥ current target OP_TS . soft deletes: either actually delete or set a IS_DELETED flag. if your source doesn’t emit OP_TS , use commit LSN/GTID/SCN as the ordering surrogate. snowflake MERGE INTO TARGET_CUSTOMERS t USING ( SELECT ID, EMAIL, OP, OP_TS FROM STG_CUSTOMERS QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) = 1 ) s ON t.ID = s.ID WHEN MATCHED AND s.OP = 'd' AND s.OP_TS = t.OP_TS THEN DELETE WHEN MATCHED AND s.OP IN ('c','u') AND s.OP_TS = t.OP_TS THEN UPDATE SET EMAIL = s.EMAIL, OP_TS = s.OP_TS WHEN NOT MATCHED AND s.OP 'd' THEN INSERT (ID, EMAIL, OP_TS) VALUES (s.ID, s.EMAIL, s.OP_TS); dedup in the USING subquery protects against duplicate events in staging. bigquery MERGE `dataset.TARGET_CUSTOMERS` t USING ( SELECT ID, EMAIL, OP, OP_TS FROM `dataset.STG_CUSTOMERS` QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) =…"
  },
  {
    "path": "/troubleshooting/",
    "title": "First 15 Minutes — CDC Troubleshooting",
    "text": "First 15 Minutes: CDC Troubleshooting A repeatable triage to stabilize incidents fast. Keep it pragmatic, measurable, and reversible. Quick Triage (5–8 minutes) Freeze changes that make state drift: pause backfills, schema changes, and connector restarts. Define the failure mode : stuck (no progress), slow (lag growing), wrong (duplicates, missing rows), or crashing . Bound the blast radius : single table/tenant vs global; snapshot vs stream. Checkpoint evidence (timestamps, connector name, offsets, group id). Don’t tail logs without writing down the top lines you see. Choose a safety : if logs are at risk of rotating out, extend retention (WAL/binlog/redo) before touching the pipeline. Goal: stop data loss, capture proof, buy time. Detailed checks below. Artifacts to Collect (Copy/Paste) Versions: source db, connector (Debezium), broker (Kafka/Redpanda), sink Connector config (sanitized): include snapshot mode, includes/excludes, heartbeat Exact error lines: 20–50 lines around the first failure Lag/offsets: consumer group lag and last committed LSN/GTID/SCN Log retention settings: WAL/binlog/redo retention and current oldest log Commands Kafka (Consumer Lag) copy kafka-consumer-gr…"
  },
  {
    "path": "/lab-kafka-debezium/",
    "title": "Hands-On Lab: CDC with Kafka, Debezium, and Postgres | CDC: The Missing Manual",
    "text": "A step-by-step guide to building a real-time Change Data Capture pipeline using Docker, Postgres, Kafka, and Debezium. Hands-On Lab: Kafka + Debezium + Postgres In this lab, you'll build a complete, end-to-end Change Data Capture (CDC) pipeline from scratch. We'll use Docker Compose to orchestrate a Postgres database, Kafka, and the Debezium Postgres connector to stream row-level changes in real time. Prerequisites Docker and Docker Compose: Ensure they are installed and running on your machine. A terminal or command prompt: Basic familiarity with shell commands. cURL or a similar tool: For interacting with the Kafka Connect REST API. Lab Setup Create Project Directory Start by creating a new folder for your lab project and navigate into it. mkdir cdc-lab && cd cdc-lab Create `docker-compose.yml` Create a file named docker-compose.yml and paste the following configuration. This file defines all the services we need: Zookeeper, Kafka, Postgres, and Debezium/Connect. # docker-compose.yml version: '3.7' services: zookeeper: image: confluentinc/cp-zookeeper:7.3.0 hostname: zookeeper container_name: zookeeper ports: - \"2181:2181\" environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_T…"
  },
  {
    "path": "/quickstarts/",
    "title": "CDC Quickstart Guides | CDC: The Missing Manual",
    "text": "Hands-on, practical Change Data Capture quickstart guides for Postgres, MySQL, and Oracle. Quickstart Guides Get a production-ready Change Data Capture pipeline running in minutes with these practical, step-by-step guides for the most common databases. Postgres Configure logical replication, set up a Debezium connector, and verify real-time change events. MySQL Enable binlog, configure a Debezium connector, and stream row-level changes to a Kafka topic. Oracle Set up supplemental logging and leverage LogMiner to capture change events in real time. MSSQL Enable native CDC, configure the SQL Server Agent, and stream changes with the Debezium connector. ← Back: Lab"
  },
  {
    "path": "/errata/",
    "title": "CDC Nuances & Errata — Read Before You Ship",
    "text": "Corrections, caveats, and sharp edges in CDC systems. Nuances & Errata A living list of precise clarifications, corrections, and “gotchas”. Treat this as pre‑flight checks before promising exactly‑once or “zero data loss.” End‑to‑end exactly‑once vs. effectively‑once Source connectors are at‑least‑once in practice. You can achieve effectively‑once by making sinks idempotent (UPSERT on stable keys) and/or deduplicating using a durable event_id ledger. Transaction boundaries : Database transactions map to change streams, not to sink transactions unless your pipeline coordinates them. Don’t assume cross‑topic atomicity. Per‑key ordering only : Ordering is guaranteed per partition key (primary/business key). Cross‑entity ordering is not guaranteed and shouldn’t be relied upon. Snapshots, backfills, and replays Initial snapshots can interleave with live changes; design consumers to reconcile using version columns or op_ts . Incremental snapshots (signal‑based) are powerful but can produce duplicates; always merge on keys + highest version and keep idempotent sinks. Backfills are just another snapshot; isolate them (topics/headers) to avoid double‑counting. Tombstones, compaction, and de…"
  },
  {
    "path": "/oracle-notes/",
    "title": "Vendor Nuances — Oracle CDC | CDC: The Missing Manual",
    "text": "Oracle Change Data Capture: prerequisites, quick checks, performance knobs, and troubleshooting playbook for log-based CDC (LogMiner/replication readers). ← Back to Quickstarts Vendor Nuances — Oracle (Log-Based CDC) Practical prerequisites, quick checks, performance knobs, and a “first-aid” section for common failures. What you must have before turning on CDC Database in ARCHIVELOG mode (recommended for CDC resiliency). Supplemental logging enabled (database-level minimal; table-level as needed for keys/columns used in merges). Privileges : read data dictionary views, redo/archive logs, target tables; ability to create log groups if required. Redo retention sized for snapshot + catch-up window (avoid log switch storms during backfills). Stable primary keys (or explicit “replica identity full” equivalents via table log groups for keyless tables). Quick Health Checks (copy/paste) Archivelog & Redo Status SELECT log_mode FROM v$database; SELECT sequence#, bytes/1024/1024 MB, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 10 ROWS ONLY; Supplemental Logging -- database-wide SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; -- table-level (co…"
  },
  {
    "path": "/everything-else/",
    "title": "extras — quickstarts, tests, and tools",
    "text": "quickstarts 🐘 postgres enable logical decoding, register Debezium, verify change events end-to-end. wal_level=logical , slots allowed PKs or REPLICA IDENTITY FULL open postgres quickstart ~10–15 min 🐬 mysql binlog in row mode, narrow includes, confirm topics receive events. binlog_format=ROW gtid_mode=ON preferred open mysql quickstart ~10–15 min 🟠 oracle check ARCHIVELOG + supplemental logging, verify redo and events. ARCHIVELOG mode DB + table supplemental logging open oracle quickstart ~15–20 min acceptance tests (shell) prove your lab is healthy with one-liners. these run on your machine (or CI) and check: stack up, connector running, events flowing, and (optional) offsets after restart. open test guide download scripts (zip) --> chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh # optional: bash scripts/test_chaos_smoke.sh manual data checks ops tests quick sanity tests that aren’t in the scripts/ folder but are essential for validation and triage. duplicate primary keys (generic SQL) -- replace table/pk SELECT COUNT(*) AS rows, COUNT(DISTINCT pk) AS distinct_keys FROM target_table; -- expect rows == distinct_keys lat…"
  },
  {
    "path": "/tests/",
    "title": "Acceptance Tests — Kafka + Debezium Lab | CDC: The Missing Manual",
    "text": "Run acceptance tests for the Kafka + Debezium lab: stack up, connector healthy, topic has change events, and (optional) chaos smoke. ← Back to Quickstarts Acceptance Tests — Kafka + Debezium Lab These tests run on your machine to prove the lab works: containers up, connector healthy, topic has change events, and (optionally) offsets advance across a restart. What You’ll Run scripts/test_stack.sh — Zookeeper, Broker, Connect, PG are running & Connect REST is reachable scripts/test_connector.sh — Connector exists and is RUNNING scripts/test_events.sh — Topic has data; consumed messages parse as JSON and include op ∈ {c,u,d} scripts/test_chaos_smoke.sh (optional) — Latest offsets increase after a controlled connector restart Requirements: docker , docker compose , curl , jq . Windows users: run under WSL or Git Bash. Run Them (Mac/Linux + WSL) chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh # optional: bash scripts/test_chaos_smoke.sh Expect green checks (✅). Any ❌ includes the failing step so you know what to fix. Windows (PowerShell via WSL) Open “Ubuntu (WSL)” cd into the lab folder you mounted ( /mnt/c/Users/you/lab ) Run…"
  }
]