[
  {
    "path": "404.html",
    "title": "page not found \u2014 cdc: the missing manual",
    "text": "page not found \u2014 cdc: the missing manual we couldn\u2019t find that page the link may be outdated or mistyped. here are quick paths to popular sections. \u2190 there's no place like home series overview open extras open lab first 15 minutes oracle notes quickstarts pick your source and follow a 10\u201320 minute setup with copy-paste checks. postgres mysql oracle common mis-typed paths if you were trying one of these, use the fixed link: /oracle \u2192 oracle notes /lab \u2192 kafka + debezium lab /first-15 \u2192 troubleshooting /quickstarts \u2192 extras hub"
  },
  {
    "path": "connector-builder.html",
    "title": "connector config builder \u2014 debezium (postgres \u2022 mysql \u2022 oracle)",
    "text": "connector config builder \u2014 debezium (postgres \u2022 mysql \u2022 oracle) \u2190 extras connector config builder generate Debezium configs for Postgres, MySQL, or Oracle. pick Debezium 2.x or 1.x, add filters & DLQ, then copy or curl to Kafka Connect. 1) choose source & version postgres mysql oracle debezium version: 2.x (topic.prefix) 1.x (database.server.name) 2) connection & naming connector name: connect url: host: port: user: password: 3) filters & behavior schema include list: table include list: topic prefix / server name: snapshot mode: initial initial_only never disable tombstones exclude schema changes DLQ topic (optional): heartbeat (ms): advanced (optional) note: toggle Debezium version above. 2.x uses topic.prefix ; 1.x uses database.server.name . choose the one your image supports. 4) generated config copy json download config.json {} 5) curl to create/update on connect copy create (POST) copy update (PUT) # POST /connectors curl -s -X POST http://localhost:8083/connectors \\ -H 'content-type: application/json' \\ -d '{ \"name\":\"inventory-connector\", \"config\": { ... } }' | jq . # PUT /connectors/{name}/config curl -s -X PUT http://localhost:8083/connectors/inventory-connector/config \\ -H 'content-type: application/json' \\ -d '{ ... }' | jq ."
  },
  {
    "path": "debezium-decoder.html",
    "title": "debezium event decoder \u2014 paste JSON, see diffs & SQL",
    "text": "debezium event decoder \u2014 paste JSON, see diffs & SQL \u2190 extras debezium event decoder paste JSON events from Kafka (Debezium or Connect DLQ). we\u2019ll detect op/source, show a before/after diff, reveal headers & errors, and generate a base MERGE/UPSERT. everything runs locally in your browser. 1) paste event(s) paste lines from kafka-console-consumer (schema-less JSON) or Connect DLQ records. multiple lines ok; non-JSON lines are ignored. try sample flatten nested fields (dot-notation) privacy: nothing leaves this page. 2) keys & sink key columns: suggest comma-separate multi-column keys (acct_id,order_id ). postgres snowflake bigquery databricks delta mysql redshift copy SQL -- paste an event to generate a MERGE/UPSERT template\u2026 we infer columns from your last event\u2019s after (or before for deletes). adjust names as needed. decoded"
  },
  {
    "path": "dlq-triage.html",
    "title": "DLQ triage assistant \u2014 find, explain, fix",
    "text": "DLQ triage assistant \u2014 find, explain, fix \u2190 extras DLQ triage assistant point this at your DLQ topic and connector name. we\u2019ll generate safe commands to inspect headers & payloads, and map common errors to fixes. 1) inputs bootstrap: dlq topic: connector name: consumer group (sink): 2) enumerate & peek copy # count a sample & show latest offsets kafka-run-class kafka.tools.GetOffsetShell \\ --broker-list localhost:29092 --topic dlq.inventory --time -1 # peek a few DLQ records (value only) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --timeout-ms 4000 \\ --max-messages 5 | jq -C . show headers & decode original value copy # show headers (kafka cli shows headers only with Kafkacat or kcat) # try kcat if available (brew install kcat) kcat -b localhost:29092 -t dlq.inventory -C -J -c 5 | jq -C . # extract original payload if record is wrapped (common in Connect DLQs) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --timeout-ms 4000 --max-messages 5 \\ | jq -r '.original.value // .record.value // .value // .payload.original.value // .payload.record.value // empty' \\ | jq -C . connector status & restart copy curl -s http://localhost:8083/connectors | jq curl -s http://localhost:8083/connectors/inventory-connector/status | jq # restart connector (use sparingly) curl -s -X POST http://localhost:8083/connectors/inventory-connector/restart sink lag (is the consumer stuck?) copy kafka-consumer-groups --bootstrap-server localhost:29092 \\ --describe --group my-sink common DLQ causes \u2192 fixes java.sql.SQLException sink rejected a row (PK/unique violation, type mismatch). \u279c check target PK & datatypes; use merge/upsert patterns. org.apache.kafka.common.errors.SerializationException value/headers not parseable (schema mismatch). \u279c align converters (schemas.enable), topic routing, or use schema registry where required. org.apache.kafka.connect.errors.ConnectException connector task error; check Connect logs; the DLQ record will carry context in headers. key not set / null key sink requires a key but topic lacks one. \u279c set proper key.converter/key.field or route to a history table, then collapse by key at sink. decode a DLQ record locally paste a DLQ JSON line below and we\u2019ll unwrap the original event. nothing is uploaded. decode open full decoder \u2192 safe re-drive (optional) once the root cause is fixed, re-drive a few DLQ records into a sandbox topic first. copy # re-drive DLQ values into a sandbox topic (no headers preserved) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --max-messages 100 \\ | jq -r '.original.value // .record.value // .value // .payload.original.value // .payload.record.value // empty' \\ | kafka-console-producer --bootstrap-server localhost:29092 --topic sandbox.replay verify sink behavior on sandbox.replay before touching the original topic."
  },
  {
    "path": "everything-else.html",
    "title": "extras \u2014 quickstarts, tests, and tools",
    "text": "extras \u2014 quickstarts, tests, and tools \u2190 back to overview extras quickstarts for Postgres/MySQL/Oracle, acceptance tests, manual data checks not covered by scripts, and helpful tools (Mermaid sandbox). quickstarts \ud83d\udc18 postgres enable logical decoding, register Debezium, verify change events end-to-end. wal_level=logical , slots allowed PKs or REPLICA IDENTITY FULL open postgres quickstart ~10\u201315 min \ud83d\udc2c mysql binlog in row mode, narrow includes, confirm topics receive events. binlog_format=ROW gtid_mode=ON preferred open mysql quickstart ~10\u201315 min \ud83d\udfe0 oracle check ARCHIVELOG + supplemental logging, verify redo and events. ARCHIVELOG mode DB + table supplemental logging open oracle quickstart ~15\u201320 min acceptance tests (shell) prove your lab is healthy with one-liners. these run on your machine (or CI) and check: stack up, connector running, events flowing, and (optional) offsets after restart. open test guide chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh # optional: bash scripts/test_chaos_smoke.sh manual data checks & ops tests quick sanity tests that aren\u2019t in the scripts/ folder but are essential for validation and triage. duplicate primary keys (generic SQL) -- replace table/pk SELECT COUNT(*) AS rows, COUNT(DISTINCT pk) AS distinct_keys FROM target_table; -- expect rows == distinct_keys latest-wins check (history vs target) -- adapt names: business key + timestamp/version WITH last AS ( SELECT key_col, MAX(op_ts) AS last_ts FROM history_table GROUP BY key_col ) SELECT COUNT(*) AS stale FROM target_table t JOIN last l ON t.key_col = l.key_col WHERE t.op_ts < l.last_ts; -- expect 0 dead-letter queue sanity (Kafka) # adjust topic; you used \"dlq.inventory\" in connector.json kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --timeout-ms 4000 \\ --max-messages 10 | jq -C . connector status (Kafka Connect REST) curl -s http://localhost:8083/connectors | jq curl -s http://localhost:8083/connectors/<name>/status | jq consumer lag (sink group) kafka-consumer-groups --bootstrap-server localhost:29092 \\ --describe --group <your-sink-group> topic offsets (sum across partitions) # inside the broker container (or use PLAINTEXT_HOST localhost:29092 externally) docker exec -it broker kafka-run-class kafka.tools.GetOffsetShell \\ --broker-list localhost:9092 --topic <topic> --time -1 tip: run these after a controlled connector restart to confirm idempotency (no dupes, no stale rows, offsets advancing). tools & vendor notes \ud83e\udde9 mermaid diagram sandbox sketch CDC dataflows, failure paths, and recovery playbooks right in the browser. open mermaid sandbox great for sharing designs \ud83d\udcd2 oracle notes prereqs, quick checks, performance knobs, and first-aid for Oracle log-based CDC. open oracle notes single-page runbook \ud83e\uddea troubleshooting the first 15 minutes: stabilize incidents fast with copy/paste diagnostics. open troubleshooting postgres \u2022 mysql \u2022 oracle \u2022 kafka \ud83e\uddf0 merge / upsert cookbook copy-paste templates for Snowflake, BigQuery, Delta, Postgres, MySQL, and Redshift. handles dupes, late events, and deletes safely. open merge cookbook latest-wins & delete-safe \ud83e\uddf0 hands-on lab copy-paste templates for Snowflake, BigQuery, Delta, Postgres, MySQL, and Redshift. handles dupes, late events, and deletes safely. open lab latest-wins & delete-safe \ud83e\uddee debezium event decoder paste a Kafka record; get op/source, before/after diff, and ready-to-tweak MERGE SQL. open decoder local only \u2022 no upload \ud83e\ude7a DLQ triage assistant enumerate, peek headers, decode original payload, and map errors to fixes. open DLQ triage safe commands \u2022 copy-ready \u2699\ufe0f connector config builder fill a form \u2192 get valid Debezium JSON + ready-to-run curl for Connect. open builder postgres \u2022 mysql \u2022 oracle \u2022 2.x/1.x"
  },
  {
    "path": "exactly-once.html",
    "title": "Exactly-Once Semantics in CDC | CDC: The Missing Manual",
    "text": "Exactly-Once Semantics in CDC | CDC: The Missing Manual Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series \ud83c\udf13 Understanding Delivery Guarantees and Exactly-Once Semantics New to CDC? Start with the CDC 101 primer . Beginner Practitioner Exactly-once, practically Networks retry; brokers retry; consumers retry. Retries \u21d2 duplicates. The thing you actually ship is at-least-once delivery + idempotent processing . Two anchors make it work: Atomicity at source : transactional outbox \u2014 write the business row and the event in one DB transaction. Idempotency at sink : MERGE/UPSERT by a stable key; ignore already-applied event IDs. Learning outcomes & success criteria By the end, you can: Explain why ALO (at-least-once) produces duplicates. Draw outbox \u2192 bus \u2192 sink with the two anchors (atomicity + idempotency). Run the lab and observe duplicates in the stream (expected). Implement an idempotent sink and pass the acceptance tests. Acceptance: Reprocessing the same event_id does not change the sink\u2019s final state. The flow at a glance App + DB Transactional Outbox Broker Idempotent Sink Dedup Ledger EOS in practice = Outbox (atomic create) + Idempotent Sink (ignore repeats). A quick demo (dup safe processing) Enter the same Order ID twice. The second is ignored by a dedup ledger. Process Order Reset Hands-on: observe ALO duplicates (lab) Have Docker? Jump to the Hands-On Lab . If it\u2019s running, prove that brokers are at-least-once by producing a few changes and consuming the stream: # Generate a few changes (inside the lab) docker exec -i pg psql -U postgres -c \"insert into app.customer(id,email,updated_at) values (gen_random_uuid(),'a@example.com',now())\" docker exec -i pg psql -U postgres -c \"update app.customer set email='b@example.com' limit 1\" docker exec -i pg psql -U postgres -c \"delete from app.customer where random() < 0.2\" # Consume 20 messages (duplicates may appear \u2014 expected in ALO) docker exec -it kafka bash -lc \"kafka-console-consumer --bootstrap-server kafka:9092 --topic dbserver1.app.customer --from-beginning --property print.key=true --max-messages 20\" Seeing duplicates here is correct. EOS is achieved at the sink using idempotency \u2014 not in the broker. Ship it: idempotent sinks (choose one) Pattern = ledger + MERGE : record processed event_id s in a ledger table and UPSERT by a stable business key. Postgres -- target table create table if not exists dw.customer( id uuid primary key, email text not null, updated_at timestamptz not null ); -- dedup ledger of processed events create table if not exists dw.processed_event( event_id text primary key, processed_at timestamptz not null default now() ); -- apply one event (params: :id, :email, :updated_at, :event_id) with dedup as ( insert into dw.processed_event(event_id) values (:event_id) on conflict do nothing returning event_id ) insert into dw.customer(id,email,updated_at) values (:id, :email, :updated_at) on conflict (id) do update set email=excluded.email, updated_at=excluded.updated_at where exists (select 1 from dedup); -- only if first time Snowflake create table if not exists DW.PROCESSED_EVENT (EVENT_ID string primary key); merge into DW.CUSTOMER as T using (select :ID as ID, :EMAIL as EMAIL, :UPDATED_AT as UPDATED_AT) as S on T.ID = S.ID when matched then update set T.EMAIL = S.EMAIL, T.UPDATED_AT = S.UPDATED_AT when not matched then insert (ID, EMAIL, UPDATED_AT) values (S.ID, S.EMAIL, S.UPDATED_AT); insert into DW.PROCESSED_EVENT(EVENT_ID) select :EVENT_ID; -- first-writer-wins BigQuery merge `dw.customer` T using (select @id as id, @email as email, @updated_at as updated_at) S on T.id = S.id when matched then update set email = S.email, updated_at = S.updated_at when not matched then insert (id, email, updated_at) values (S.id, S.email, S.updated_at); insert into `dw.processed_event` (event_id, processed_at) values (@event_id, current_timestamp()); Consumer skeleton // at-least-once + idempotent sink for (event of stream) { if (dedup.contains(event.id)) continue; // already applied tx.begin(); sink.merge(key=event.key, payload=event.payload); // UPSERT by business key dedup.put(event.id); // record after sink write tx.commit(); } EOS Readiness Checklist Mark what\u2019s true in your environment. The summary tells you whether you can safely claim \u201cexactly-once (practically)\u201d today. Source (Atomicity) Transactional outbox: business row + event written in the same DB transaction Each event has a stable event_id (UUID or monotonic ID) Transport (Tolerate ALO) Producers/consumers may retry; duplicates on the bus are acceptable Partitioning preserves per-entity order (good key choice) Sink (Idempotency) Apply via MERGE/UPSERT on a stable business key Dedup ledger records processed event_id s (first-writer-wins) Deletes handled explicitly (tombstones / soft-delete semantics) Ops (Replay & Monitoring) Replays are safe (idempotent sink verified) SLIs: dup_rate , drop_rate , lag_seconds are tracked Failure-Mode Planner Toggle likely failures. The planner explains what must hold for EOS to survive each one. Producer retries the same commit Broker re-delivers messages Consumer crashes mid-apply then retries Events arrive out of order within the partition Acceptance tests: prove EOS in practice Run this in the lab\u2019s Postgres to black-box verify idempotency: docker exec -i pg psql -U postgres <<'SQL' create schema if not exists dw; create table if not exists dw.customer(id uuid primary key, email text, updated_at timestamptz); create table if not exists dw.processed_event(event_id text primary key, processed_at timestamptz default now()); -- same event delivered twice (e1) \u2192 second apply ignored with d as (insert into dw.processed_event(event_id) values ('e1') on conflict do nothing returning 1) insert into dw.customer(id,email,updated_at) values ('00000000-0000-0000-0000-000000000001','first@example.com', now()) on conflict (id) do update set email=excluded.email, updated_at=excluded.updated_at where exists (select 1 from d); with d as (insert into dw.processed_event(event_id) values ('e1') on conflict do nothing returning 1) insert into dw.customer(id,email,updated_at) values ('00000000-0000-0000-0000-000000000001','second@example.com', now()) on conflict (id) do update set email=excluded.email, updated_at=excluded.updated_at where exists (select 1 from d); -- new event (e2) legitimately updates the row with d as (insert into dw.processed_event(event_id) values ('e2') on conflict do nothing returning 1) insert into dw.customer(id,email,updated_at) values ('00000000-0000-0000-0000-000000000001','third@example.com', now()) on conflict (id) do update set email=excluded.email, updated_at=excluded.updated_at where exists (select 1 from d); table dw.customer; table dw.processed_event; SQL Expected: dw.customer.email ends as third@example.com ; dw.processed_event has e1 and e2 once each. Transactional Outbox \u2014 Mermaid Diagram Common pitfalls (and the fixes) Unstable keys \u2192 use a stable business key (or immutable surrogate) for MERGE. Ledger bloat \u2192 compact by time/window; or keep recent IDs if keys are monotonic. Reordering \u2192 partition by entity key; add version/timestamp guards if needed. Deletes \u2192 emit tombstones; sinks must handle delete semantics explicitly. Replays \u2192 idempotent sinks make replays safe; verify with the acceptance tests. Recap: the end-to-end recipe Source : transactional outbox (row + event in one commit). Transport : tolerate ALO duplicates; don\u2019t assume broker-level EOS. Sink : MERGE/UPSERT by key + dedup ledger; pass the acceptance tests. \u2190 Back to Intro to CDC Next: Multi-Tenancy \u2192 \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "index.html",
    "title": "CDC: The Missing Manual | A Deep Dive into Change Data Capture",
    "text": "CDC: The Missing Manual | A Deep Dive into Change Data Capture Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series \ud83c\udf13 Why Change Data Capture Still Breaks, and How To Get It Right. Your analytics team needs data in real-time. Your business partners want to build dashboards, set up alerts, and tailor customer experiences on the fly. CDC seems like the answer, but most implementations fail. This series shows how to get it right. What is Change Data Capture (CDC)? CDC streams row-level changes from a source database to downstream systems in near real time. Modern platforms do this by reading the database\u2019s transaction log (log-based CDC), transforming each change into an event, and delivering it to one or more sinks (data lake/warehouse, caches, search, services). How it works (at a glance) Source DB writes to its transaction log. Log reader/agent tails the log and emits change events. Broker/stream transports events (Kafka). Sinks consume and apply changes (MERGE/UPSERT). Why teams adopt CDC Keep analytics/search/caches fresh without heavy batch jobs. Decouple systems with events instead of direct DB reads. Enable real-time features (alerts, personalization, audit trails). Start with the interactive intro \u2192 Understand \u201cExactly-Once\u201d \u2192 What You\u2019ll Learn Exactly-Once Semantics Why the thing you actually ship is at-least-once + idempotency , not mythical global EOS. Transactional outbox anchors atomicity Idempotent sinks (MERGE/UPSERT) drop dupes Backfill \u2192 cutover \u2192 reconcile loop Interactive diagram Beginner \u2194 Practitioner Multi-Tenancy Isolation vs cost levers that keep SRE and Finance happy. Shared vs per-tenant topics vs per-tenant clusters Throughput math: tenants \u00d7 rate \u00d7 (payload+envelope) Guardrails: quotas, DLQ policy, tenant SLAs Calculator Beginner \u2194 Practitioner Partitioning Keys, skew, late arrivals \u2014 and how to prove correctness. Choosing keys to preserve per-entity order Detecting & taming partition skew Reconciliation: SLIs for dup/drop/late rates Coming next Schema evolution : mapping, defaults, backfills, schema-ID pinning Auditing & lineage : proofs, replay safety, CDC vs. GDPR deletes Observability : lag, dup/drop rate, DLQs, replay dashboards \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "intro.html",
    "title": "Interactive Intro to CDC | CDC: The Missing Manual",
    "text": "Interactive Intro to CDC | CDC: The Missing Manual Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series \ud83c\udf13 Interactive Introduction to CDC Beginner Practitioner Change Data Capture ( CDC ) is the discipline of replicating data changes from a source database to downstream systems in near real time\u2014without heavy full refreshes. CDC methods Log-based (preferred): tail the DB\u2019s transaction log (WAL/binlog/redo). Low overhead, faithful ordering. Trigger-based : DB triggers write to outbox/staging tables. Simpler to start; adds write overhead and schema coupling. Polling : periodically diff tables/columns with watermarks. Easiest but highest lag/load. Core components Source DB (+ its log) Log reader / connectors ( Debezium agents) Transport (Kafka, Kinesis, Pub/Sub) Sinks (data lake/warehouse, search, caches, services) Delivery & idempotency Most real systems are at-least-once . You get duplicates during retries. The practical recipe for \u201cexactly-once\u201d behavior is: Transactional outbox + idempotent sink . // Pseudocode: idempotent consumer for (event of stream) { if (dedup_store.contains(event.id)) continue; // drop duplicate upsert_into_sink(event.key, event.payload); // MERGE/UPSERT dedup_store.put(event.id); // commit after sink write } Operational gotchas Schema evolution : map types, manage defaults/backfills, pin schema IDs. Out-of-order : preserve per-entity ordering with a good partition key. Backfills/cutover : snapshot once, then switch to streaming; reconcile deltas. Multi-tenancy : shared vs per-tenant topics/clusters; quotas and RBAC. See delivery guarantees \u2192 Try the multi-tenancy calculator \u2192 Source DB Log Reader/Agent Broker Warehouse Search/Cache/Apps Log-based CDC: DB \u2192 log reader \u2192 broker \u2192 sinks. Why CDC Matters: The End of Data Lag Real-Time Analytics Power dashboards with the freshest data, enabling stakeholders to spot trends and react to issues the moment they happen. Data Consistency Act as a synchronization fabric, keeping caches, search indexes, and microservices consistently up-to-date. Reduced System Load Dramatically reduce performance impact on source systems compared to inefficient bulk data transfers or polling. Modern Architectures Enable zero-downtime migrations, event-driven systems, and reliable data exchange in microservices. The How: Core CDC Methodologies There are three primary ways to implement CDC, each with significant trade-offs in performance, reliability, and complexity. This interactive visualization compares them across key dimensions to show why log-based CDC is the gold standard for modern systems. Comparing CDC Approaches Architectural Blueprints: The Transactional Outbox CDC is a foundational component for modern systems. This section demonstrates the most important architectural pattern enabled by CDC: the Transactional Outbox. Click through the steps to see how it ensures reliable data exchange between microservices without distributed transactions. 1. Atomic Write Within a single database transaction, the Application Service writes business data to its main table AND inserts a corresponding event message into an `outbox` table. 2. Log-Based CDC A CDC tool (Debezium) monitors the database's transaction log. It is configured to watch *only* the `outbox` table for new entries. 3. Event Publishing Upon detecting a committed `INSERT` to the `outbox` table, the CDC tool reads the event message and reliably publishes it to a message bus like Apache Kafka. 4. Consumption A downstream microservice (a Notification service) subscribes to the Kafka topic, consumes the event, and performs its own business logic. \ud83c\udfac Ready to start Start Animation The CDC Tooling Ecosystem Picking CDC tooling is a trade-off across capture method (prefer log-based), ops model (managed vs self-managed), source/target coverage & versions , and exactly-once behavior at the sink (idempotent upserts + replay). The boxes below are vendor-neutral and non-exhaustive\u2014verify current docs before committing. CDC platforms Reset Debezium + Kafka Connect Open-source log-based CDC connectors. Flexible; you operate Kafka/Connect or use managed Kafka. open source log-based DIY ops Matillion (Data Loader / Designer) Wizard-driven CDC into cloud warehouses plus orchestration/transform in one place. managed log-based ELT + orchestration Fivetran Managed connectors with log-based CDC options; emphasizes quick setup and hands-off operations. managed log-based Precisely (Connect) Enterprise replication including mainframe/legacy into modern targets. enterprise log-based heterogeneous Qlik Replicate (Attunity) UI-driven enterprise CDC across a wide range of sources/targets. enterprise log-based Oracle GoldenGate Oracle\u2019s CDC/replication stack (supports several non-Oracle endpoints) for mission-critical use. enterprise log-based AWS DMS Managed migrations and ongoing CDC into AWS targets (and beyond via connectors). managed log-based Google Cloud Datastream Serverless CDC for MySQL/Postgres/Oracle streaming into BigQuery/Cloud Storage. serverless log-based Confluent Commercial Connectors Kafka Connect ecosystem with additional commercial CDC connectors (Oracle). commercial log-based StreamSets Streaming pipelines with CDC connectors into warehouses/lakes. platform CDC connectors Striim CDC + in-flight SQL-like processing for real-time ops analytics. platform CDC + transform IBM Data Replication Log-based CDC for Db2 and mixed mainframe/distributed estates. enterprise log-based SAP SLT Native trigger/log-based replication from SAP ECC/S/4HANA to downstream stores. SAP trigger/log Hevo Data Managed ELT with CDC support into cloud targets. managed CDC Airbyte Open-source ELT; CDC varies by connector (check docs for log-based maturity per source). open source mixed CDC Selection checklist Capture Prefer log-based. Confirm DB version/edition, permissions, topology (RDS replicas, RAC, Always On). Delivery Order/partitioning guarantees, dead-letter handling, replay strategy. Sink semantics Upsert/merge idempotency; schema evolution; reconcile late arrivals. Ops model SLAs, monitoring, cost, change windows, failover/DR. Security Network path, secrets, encryption, row-level filtering/PII. \u2190 Back: Series Overview Next: Exactly-Once \u2192 \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "lab-kafka-debezium.html",
    "title": "Hands-On Lab: Kafka + Debezium + Postgres | CDC: The Missing Manual",
    "text": "Hands-On Lab: Kafka + Debezium + Postgres | CDC: The Missing Manual home the series lab lab: kafka + debezium + postgres start a local cdc stack (postgres \u2192 debezium \u2192 kafka) and watch change events end-to-end. quickstart is five commands. quickstart: 5 commands start the stack docker compose up -d seed the database docker exec -i pg psql -U postgres -f /init.sql register the Debezium connector curl -s -X PUT http://localhost:8083/connectors/inventory-connector/config \\ -H 'content-type: application/json' \\ -d @connector.json | jq generate changes docker exec -i pg psql -U postgres -c \"insert into app.customer(id,email,updated_at) values (gen_random_uuid(),'a@example.com',now())\" docker exec -i pg psql -U postgres -c \"update app.customer set email='b@example.com' limit 1\" docker exec -i pg psql -U postgres -c \"delete from app.customer where random() < 0.2\" watch events docker exec -it broker kafka-console-consumer \\ --bootstrap-server broker:9092 \\ --topic server1.public.app_customer \\ --from-beginning files (copy these beside this html) docker-compose.yml services: zookeeper: image: confluentinc/cp-zookeeper:7.6.0 environment: ZOOKEEPER_CLIENT_PORT: 2181 ports: [\"2181:2181\"] broker: image: confluentinc/cp-kafka:7.6.0 ports: [\"9092:9092\",\"29092:29092\"] environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 depends_on: [zookeeper] pg: image: debezium/example-postgres:2.6 environment: POSTGRES_PASSWORD: postgres POSTGRES_DB: postgres ports: [\"5432:5432\"] volumes: - ./init.sql:/init.sql connect: image: debezium/connect:2.6 ports: [\"8083:8083\"] environment: BOOTSTRAP_SERVERS: broker:9092 GROUP_ID: 1 CONFIG_STORAGE_TOPIC: connect-configs OFFSET_STORAGE_TOPIC: connect-offsets STATUS_STORAGE_TOPIC: connect-status KEY_CONVERTER_SCHEMAS_ENABLE: \"false\" VALUE_CONVERTER_SCHEMAS_ENABLE: \"false\" depends_on: [broker] init.sql create schema if not exists app; create extension if not exists pgcrypto; create table if not exists app.customer ( id uuid primary key, email text not null, updated_at timestamp not null default now() ); insert into app.customer(id,email,updated_at) select gen_random_uuid(), concat('user', i, '@example.com'), now() from generate_series(1,10) as s(i) on conflict do nothing; connector.json { \"name\": \"inventory-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"database.hostname\": \"pg\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"postgres\", \"database.dbname\": \"postgres\", \"topic.prefix\": \"server1\", \"schema.include.list\": \"app\", \"table.include.list\": \"app.customer\", \"slot.name\": \"cdc_slot\", \"publication.autocreate.mode\": \"filtered\", \"tombstones.on.delete\": \"false\", \"heartbeat.interval.ms\": \"5000\", \"heartbeat.topics.prefix\": \"heartbeat\", \"decimal.handling.mode\": \"string\", \"include.schema.changes\": \"false\", \"snapshot.mode\": \"initial\", \"errors.tolerance\": \"all\", \"errors.deadletterqueue.topic.name\": \"dlq.inventory\", \"errors.deadletterqueue.context.headers.enable\": \"true\" } } idempotent sink (optional) for a true end-to-end \u201ceffectively once\u201d flow, your sink should treat replays as no-ops. here\u2019s a template merge for Postgres: insert into target_customers (id, email, op_ts, version) values ($1,$2,$3,$4) on conflict (id) do update set email = excluded.email, op_ts = greatest(target_customers.op_ts, excluded.op_ts), version = greatest(target_customers.version, excluded.version); watch events you should see Debezium\u2019s change events for inserts, updates, and deletes on the server1.public.app_customer topic. bonus: chaos test your idempotency generate continuous updates, bounce components mid-stream, then verify the target has no duplicate primary keys and reflects the latest event per key. 1) start a change generator (postgres) docker exec -i pg psql -U postgres -d postgres -v \"ON_ERROR_STOP=1\" <<'SQL' CREATE EXTENSION IF NOT EXISTS pgcrypto; -- ensure at least 10 rows exist INSERT INTO app.customer(id,email) SELECT gen_random_uuid(), concat(floor(random()*1000000)::text,'@example.com') FROM generate_series(1,10) ON CONFLICT DO NOTHING; -- churn updates on hot keys for ~2.5 minutes DO $$ DECLARE i INT := 0; BEGIN WHILE i < 1000 LOOP UPDATE app.customer SET email = encode(gen_random_bytes(4),'hex') || '@example.com', updated_at = NOW() WHERE id IN (SELECT id FROM app.customer LIMIT 1 OFFSET floor(random()*10)); PERFORM pg_sleep(0.15); i := i + 1; END LOOP; END$$; SQL 2) while updates flow, bounce components # restart the source connector task curl -s -X POST http://localhost:8083/connectors/inventory-connector/restart # optional: restart connect worker (harder failure) docker compose restart connect # optional: restart the broker to force producer retries docker compose restart broker 3) verify the sink is idempotent generic duplicate / latest-wins checks -- duplicates: rows == distinct primary keys SELECT COUNT(*) AS rows, COUNT(DISTINCT id) AS distinct_keys FROM target_customers; -- latest-wins (if you keep a history table) WITH last AS ( SELECT id, MAX(op_ts) AS last_ts FROM history_customers GROUP BY id ) SELECT COUNT(*) FROM target_customers t JOIN last l USING(id) WHERE t.op_ts < l.last_ts; -- expect 0 acceptance no duplicate primary keys after restarts ( rows == distinct_keys ). replaying the last N events produces 0 changes in the target. connector offsets/LSN advance past the restart time. verify with acceptance tests run these to prove your lab is healthy (containers up, connector running, events flowing). chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh read the test guide \u2192 cleanup docker compose down -v \u2190 back home next: series overview \u2192"
  },
  {
    "path": "merge-cookbook.html",
    "title": "merge / upsert cookbook \u2014 sinks for CDC",
    "text": "merge / upsert cookbook \u2014 sinks for CDC \u2190 back to overview merge / upsert cookbook templates you can drop into your sinks to make CDC replays idempotent, handle late events, and apply deletes safely. shared assumptions (adapt to your schema) staging table: STG_CUSTOMERS (raw CDC) with columns ID , EMAIL , OP (c/u/d), OP_TS (event time), optional VERSION or LSN/GTID/SCN . target table: TARGET_CUSTOMERS (latest state). business key: ID . replace with your key columns as needed. late events: we accept an incoming row if its OP_TS \u2265 current target OP_TS . soft deletes: either actually delete or set a IS_DELETED flag. if your source doesn\u2019t emit OP_TS , use commit LSN/GTID/SCN as the ordering surrogate. snowflake MERGE INTO TARGET_CUSTOMERS t USING ( SELECT ID, EMAIL, OP, OP_TS FROM STG_CUSTOMERS QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) = 1 ) s ON t.ID = s.ID WHEN MATCHED AND s.OP = 'd' AND s.OP_TS >= t.OP_TS THEN DELETE WHEN MATCHED AND s.OP IN ('c','u') AND s.OP_TS >= t.OP_TS THEN UPDATE SET EMAIL = s.EMAIL, OP_TS = s.OP_TS WHEN NOT MATCHED AND s.OP <> 'd' THEN INSERT (ID, EMAIL, OP_TS) VALUES (s.ID, s.EMAIL, s.OP_TS); dedup in the USING subquery protects against duplicate events in staging. bigquery MERGE `dataset.TARGET_CUSTOMERS` t USING ( SELECT ID, EMAIL, OP, OP_TS FROM `dataset.STG_CUSTOMERS` QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) = 1 ) s ON t.ID = s.ID WHEN MATCHED AND s.OP = 'd' AND s.OP_TS >= t.OP_TS THEN DELETE WHEN MATCHED AND s.OP IN ('c','u') AND s.OP_TS >= t.OP_TS THEN UPDATE SET EMAIL = s.EMAIL, OP_TS = s.OP_TS WHEN NOT MATCHED AND s.OP != 'd' THEN INSERT (ID, EMAIL, OP_TS) VALUES (s.ID, s.EMAIL, s.OP_TS); consider partitioning TARGET_CUSTOMERS by DATE(OP_TS) and clustering by ID for merge performance. databricks delta MERGE INTO target_customers AS t USING ( SELECT ID, EMAIL, OP, OP_TS FROM stg_customers QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) = 1 ) AS s ON t.ID = s.ID WHEN MATCHED AND s.OP = 'd' AND s.OP_TS >= t.OP_TS THEN DELETE WHEN MATCHED AND s.OP IN ('c','u') AND s.OP_TS >= t.OP_TS THEN UPDATE SET t.EMAIL = s.EMAIL, t.OP_TS = s.OP_TS WHEN NOT MATCHED AND s.OP <> 'd' THEN INSERT (ID, EMAIL, OP_TS) VALUES (s.ID, s.EMAIL, s.OP_TS); enable OPTIMIZE + ZORDER BY (ID) on large tables to keep merge fast. postgres -- ensure pk ALTER TABLE target_customers ADD PRIMARY KEY (id); -- idempotent upsert with latest-wins INSERT INTO target_customers (id, email, op_ts) SELECT id, email, op_ts FROM ( SELECT id, email, op, op_ts, ROW_NUMBER() OVER (PARTITION BY id ORDER BY op_ts DESC) AS rn FROM stg_customers ) s WHERE s.rn = 1 AND s.op <> 'd' ON CONFLICT (id) DO UPDATE SET email = EXCLUDED.email, op_ts = GREATEST(target_customers.op_ts, EXCLUDED.op_ts); -- deletes DELETE FROM target_customers t USING ( SELECT id, MAX(op_ts) AS op_ts FROM stg_customers WHERE op = 'd' GROUP BY id ) d WHERE t.id = d.id AND d.op_ts >= t.op_ts; two-step approach (upsert then delete) is simple and fast; wrap in a transaction. mysql -- latest non-delete per id into a temp table CREATE TEMPORARY TABLE tmp_latest AS SELECT id, email, op, op_ts FROM ( SELECT id, email, op, op_ts, ROW_NUMBER() OVER (PARTITION BY id ORDER BY op_ts DESC) rn FROM stg_customers ) x WHERE rn = 1; -- upsert INSERT INTO target_customers (id, email, op_ts) SELECT id, email, op_ts FROM tmp_latest WHERE op <> 'd' ON DUPLICATE KEY UPDATE email = VALUES(email), op_ts = GREATEST(target_customers.op_ts, VALUES(op_ts)); -- delete DELETE t FROM target_customers t JOIN tmp_latest d ON d.id = t.id AND d.op = 'd' AND d.op_ts >= t.op_ts; ensure an index/PK on target_customers(id) . MySQL 8+ window functions simplify the dedupe step. redshift -- staging dedupe (late-events safe) CREATE TEMP TABLE stg_dedup DISTKEY(id) SORTKEY(id) AS SELECT id, email, op, op_ts FROM ( SELECT id, email, op, op_ts, ROW_NUMBER() OVER (PARTITION BY id ORDER BY op_ts DESC) rn FROM stg_customers ) s WHERE rn = 1; -- deletes first (to avoid extra writes) DELETE FROM target_customers t USING stg_dedup d WHERE d.op = 'd' AND t.id = d.id AND d.op_ts >= t.op_ts; -- upsert via MERGE (supported) MERGE INTO target_customers t USING stg_dedup s ON t.id = s.id WHEN MATCHED AND s.op <> 'd' AND s.op_ts >= t.op_ts THEN UPDATE SET email = s.email, op_ts = s.op_ts WHEN NOT MATCHED AND s.op <> 'd' THEN INSERT (id, email, op_ts) VALUES (s.id, s.email, s.op_ts); consider VACUUM / ANALYZE schedules on heavy churn tables. handling hard parts 1) replays after a crash -- safe replay: re-run the last N minutes of staging DELETE FROM target_customers WHERE (id, op_ts) IN ( SELECT id, op_ts FROM stg_customers WHERE op_ts >= NOW() - INTERVAL '10 minutes' ); -- re-run the regular upsert + delete logic 2) keyless tables avoid \u201creplica identity full\u201d targets. pick a **natural business key** or synthesize one (hash of stable columns). worst case, land to a history table only. 3) soft deletes vs hard deletes -- soft delete variant (Snowflake example) WHEN MATCHED AND s.OP = 'd' AND s.OP_TS >= t.OP_TS THEN UPDATE SET IS_DELETED = TRUE, OP_TS = s.OP_TS 4) schema evolution additive columns are easiest: default target to NULL and include them in the UPDATE SET . for type changes, land to a compatible staging column and cast during merge. acceptance checks (copy/paste) -- duplicates: expect 0 difference SELECT COUNT(*) AS rows, COUNT(DISTINCT id) AS keys FROM target_customers; -- latest-wins: target should reflect max OP_TS per id (expect 0 stale) WITH last AS (SELECT id, MAX(op_ts) AS last_ts FROM stg_customers GROUP BY id) SELECT COUNT(*) FROM target_customers t JOIN last l USING(id) WHERE t.op_ts < l.last_ts; -- delete sanity (expect 0) SELECT COUNT(*) FROM target_customers t JOIN ( SELECT id, MAX(op_ts) op_ts FROM stg_customers WHERE op='d' GROUP BY id ) d USING(id) WHERE t.op_ts < d.op_ts;"
  },
  {
    "path": "mermaid-sandbox.html",
    "title": "Mermaid Sandbox",
    "text": "Mermaid Sandbox sequenceDiagram autonumber participant App as App Service participant DB as OLTP DB participant OB as Outbox Table participant Bus as Event Bus participant Sink as Consumer App->>DB: BEGIN TX App->>DB: UPDATE domain rows App->>OB: INSERT event record DB-->>App: COMMIT OB-->>Bus: Outbox relay publishes Bus-->>Sink: Consume and apply (idempotent)"
  },
  {
    "path": "multi-tenancy.html",
    "title": "Multi-Tenancy \u2014 Isolation vs Cost | CDC: The Missing Manual",
    "text": "Multi-Tenancy \u2014 Isolation vs Cost | CDC: The Missing Manual Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series \ud83c\udf13 Multi-Tenancy: Cost vs Isolation Beginner Practitioner Primer: Why multi-tenancy matters for CDC Sharing infrastructure saves money but couples tenants together. More isolation (per-tenant topics or clusters) reduces blast radius and gives clearer SLAs, but increases cost and operational load. Isolation ladder: shared topics \u2192 per-tenant topics \u2192 per-tenant clusters. Throughput \u2248 tenants \u00d7 change_rate \u00d7 (payload + envelope) ; budget egress and quotas. RBAC/retention easier per-tenant topics; cluster isolation for regulators/VIPs. Guardrails: quotas, DLQ policy, per-tenant error budgets, topic-level ACLs. Rule-of-thumb cutovers \u2191 tenants & low compliance \u21d2 shared topics until ops pain shows. \u2191 compliance / RBAC / per-tenant SLAs \u21d2 per-tenant topics. Regulatory isolation / VIPs / heavy tenants \u21d2 per-tenant clusters. Use the controls to explore topic counts, consumer groups, egress, and agent footprint. 1. Adjust Your Scenario Preset: Startup Preset: Regulated Enterprise Preset: Noisy Neighbor Copy link Controls Tenants 10 Independent customers served. Change rate (events/s/tenant) 100 Avg payload (bytes) 500 Envelope overhead (bytes) ? Headers, schema IDs, metadata. Typical 200\u2013400B for JSON/Avro. 300 Topics per tenant (per service) ? outbox, status, DLQ, metadata\u2026 Adjust to your design. 6 Shared topics (if shared mode) 6 Isolation model Shared topics Per-tenant topics Per-tenant clusters Shared = cheapest; Topics = simpler RBAC; Clusters = strongest isolation. 2. See the Impact Total Topics \u2014 Shared: fixed; per-tenant: tenants \u00d7 topics_per_tenant . Consumer Groups \u2014 \u2248 tenants \u00d7 GROUPS_PER_TENANT . Egress (MB/s) \u2014 Decimal MB (bytes \u00f7 1e6). Egress (GB/month) \u2014 60\u00d760\u00d724\u00d730 seconds. Connector Footprint \u2014 Per-cluster \u2248 1:1; others scale with tenants. This index is a first-order illustration: egress (MB/s) + connector footprint (+ topic management) . Tune constants to your platform. How we estimate egress_bytes_per_s = tenants \u00d7 change_rate \u00d7 (payload + envelope) consumer_groups \u2248 tenants \u00d7 GROUPS_PER_TENANT Topics: Shared : shared_topics Per-tenant topics : tenants \u00d7 topics_per_tenant Per-tenant clusters : total topics = tenants \u00d7 topics_per_tenant Assumptions & levers Consumer groups factor GROUPS_PER_TENANT (default 2). Connector footprint: shared/per-topic \u2248 tenants \u00d7 0.2; per-cluster \u2248 1:1. Envelope defaults for JSON/Avro headers (200\u2013400B typical). RBAC/retention overhead grows with topic count, not egress. Decision helper (rule-of-thumb thresholds) Shared topics until: topics \u2264 ~50, low compliance, dup-tolerant consumers. Per-tenant topics when: per-tenant RBAC/retention, > ~50 topics total, noisy neighbors. Per-tenant clusters when: regulated isolation, VIPs, or tenant egress \u2265 ~10% of fleet. Heuristics; validate with SRE/Compliance. FAQ \u2014 multi-tenant CDC Do per-tenant topics improve security? Yes\u2014RBAC is simpler and audits map to tenants, but cost/metadata overhead rises. If regulators require isolation, consider cluster boundaries. Will per-tenant clusters always cost more? Typically yes (1:1 footprint), but noisy tenants stop taxing others and maintenance windows become tenant-scoped. \u2190 Back: Exactly-Once Next: Partitioning \u2192 \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "oracle-notes.html",
    "title": "vendor nuances \u2014 oracle (log-based cdc)",
    "text": "vendor nuances \u2014 oracle (log-based cdc) \u2190 back to home vendor nuances \u2014 oracle (log-based cdc) practical prerequisites, quick checks, performance knobs, and a \u201cfirst-aid\u201d section for common failures. what you must have before turning on CDC database in ARCHIVELOG mode (recommended for CDC resiliency). supplemental logging enabled (database-level minimal; table-level as needed for keys/columns used in merges). privileges : read data dictionary views, redo/archive logs, target tables; ability to create log groups if required. redo retention sized for snapshot + catch-up window (avoid log switch storms during backfills). stable primary keys (or explicit \u201creplica identity full\u201d equivalents via table log groups for keyless tables). quick health checks (copy/paste) archivelog & redo status SELECT log_mode FROM v$database; -- expect ARCHIVELOG SELECT sequence#, bytes/1024/1024 MB, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 10 ROWS ONLY; supplemental logging -- database-wide SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; -- table-level (confirm keys/columns logged for merges on keyless tables) SELECT owner, table_name, log_group_name, always, log_group_type FROM dba_log_groups ORDER BY owner, table_name; current redo pressure SELECT TO_CHAR(first_time,'YYYY-MM-DD HH24:MI') AS first_time, COUNT(*) AS switches FROM v$log_history WHERE first_time > SYSDATE - 1/24 -- last hour GROUP BY TO_CHAR(first_time,'YYYY-MM-DD HH24:MI') ORDER BY 1 DESC; connector snapshot sanity scope restricted: include only necessary schemas/tables. long-running transactions identified (they can stall snapshot start). redo retention \u2265 expected snapshot duration. if tables lack PKs, ensure table-level log groups capture enough columns for deterministic merges. nice-to-have SQL for big snapshots -- largest tables by size SELECT owner, segment_name, bytes/1024/1024 MB FROM dba_segments WHERE segment_type='TABLE' ORDER BY bytes DESC FETCH FIRST 20 ROWS ONLY; performance knobs (choose carefully, measure after each change) redo log size & count: larger logs reduce switch frequency during snapshots; too large can delay archiving. filter scope: narrow includes to reduce LogMiner workload and topic chatter. task/worker parallelism: increase only if the source and sink can keep up; watch redo/CPU. batch/fetch sizing: tune connector fetch/commit sizes to match sink merge throughput. LOB handling: consider excluding or routing large LOB tables to a separate flow; they dominate redo and bandwidth. common failure \u2192 first aid duplicates after restarts: switch the sink to idempotent MERGE/UPSERT keyed by a stable id + version/op_ts; then replay last N events. missed updates: enable table-level supplemental logging for key columns (or all columns for keyless) and re-snapshot. dictionary mismatch / DDL errors: pause, capture the DDL, align connector schema evolution settings, then resume from a safe SCN. redo switch storm: reduce snapshot scope or increase redo log size; avoid concurrent heavy maintenance jobs. SCN gaps / \u201clog not found\u201d: extend archive retention; if a gap exists, plan a clean re-snapshot of affected tables. observability expose SCN / commit timestamp per event (headers/columns) all the way to the sink. emit connector task metrics (lag, batches/sec, records/sec) and alert on flatlining progress. dead-letter queue (DLQ) enabled with payload + error string for triage. duplicate / latest-wins checks (sink) -- generic duplicate check (replace target & pk) SELECT COUNT(*) AS rows, COUNT(DISTINCT pk) AS distinct_keys FROM target; -- latest-wins (replace names) SELECT business_id, MAX(op_ts) AS last_seen, COUNT(*) AS events FROM history_or_stage GROUP BY business_id; upgrade checklist read the connector release notes for breaking changes (catalog strategies, snapshot defaults, DDL handling). test on a staging database with archived logs; verify no duplicate PKs after a controlled restart. capture before/after redo switch rates and connector lag to spot regressions early. acceptance criteria database in ARCHIVELOG ; supplemental logging verified at DB and required tables snapshot finishes without schema conflicts; archived logs cover entire snapshot window controlled connector restart produces no duplicate PKs in the sink steady-state redo switch rate and connector lag are within SLOs DLQ empty (or only known test errors) for 30 minutes"
  },
  {
    "path": "overview.html",
    "title": "CDC: The Missing Manual | Series Overview",
    "text": "CDC: The Missing Manual | Series Overview Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series Extras \ud83c\udf13 Series Overview From -event demos to production-grade pipelines. Follow the modules below to master delivery guarantees, multi\u2011tenant trade\u2011offs, and correctness at scale. Interactive Introduction to CDC An interactive dashboard covering core concepts, methods, architectures, and the tooling ecosystem. Dive In! Exactly\u2011Once Semantics Visual walkthrough of ALO vs EOS + transactional outbox. Dive In! Multi\u2011Tenancy Isolation patterns, topic math, and rough egress estimates. Dive In! Partitioning Partition keys, skew, late\u2011arrivals, and audit loops. Dive In! \u2190 Previous Next: Intro to CDC \u2192 \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "partitioning.html",
    "title": "Partitioning & Reconciliation \u2014 Keys, Skew, Late Arrivals | CDC: The Missing Manual",
    "text": "Partitioning & Reconciliation \u2014 Keys, Skew, Late Arrivals | CDC: The Missing Manual Skip to content CDC: The Missing Manual | A Deep Dive into Change Data Capture Home The Series \ud83c\udf13 Partitioning & Reconciliation \u2014 Keys, Skew, Late Arrivals Beginner Practitioner Why this matters Brokers preserve per-partition order , not global order. Your partition key determines ordering guarantees and load distribution. Reconciliation logic in the sink must absorb duplicates, tolerate out-of-order arrivals, and converge to the correct final state. Key choice \u21d2 per-entity order, fan-out, and skew. Skew \u21d2 hot keys slow consumers, inflate lag. Late arrivals \u21d2 without version guards, you can revert state. Partitioning simulator (order, skew, lag) Controls Partitions 6 Distinct keys (entities) 1000 Events / sec (fleet) 5000 Key distribution Uniform Zipf (skewed) Hot key (x% to 1 key) Skew stresses single partitions. If \u201cHot\u201d: % of traffic to 1 key 40% Consumer capacity per partition (events/s) 2000 Max partition rate \u2014 Events/s on the hottest partition. P95 partition rate \u2014 Load tail\u2014capacity planning. Estimated lag \u2014 Seconds to drain if max > capacity. Ordering guarantee Per-key Global order is not guaranteed. Partition load (events/s) Tip: If one bar dominates, consider a better key (include tenant or sharding suffix), or move hot tenants to their own topic/cluster. Reconciliation patterns (make late arrivals safe) Idempotency prevents duplicates from changing state. To protect against late arrivals and reordering, add a version guard and optionally a watermark window. Postgres pattern: MERGE + version guard -- target table includes a monotonic version (event time or seq) create table if not exists dw.customer( id uuid primary key, email text not null, version bigint not null, -- or timestamptz updated_at timestamptz not null ); -- dedup ledger as in EOS page (event_id primary key) create table if not exists dw.processed_event( event_id text primary key, processed_at timestamptz not null default now() ); -- apply event only if it's not a duplicate AND version is newer with dedup as ( insert into dw.processed_event(event_id) values (:event_id) on conflict do nothing returning 1 ) insert into dw.customer(id,email,version,updated_at) values (:id,:email,:version,now()) on conflict (id) do update set email = excluded.email, version = excluded.version, updated_at = excluded.updated_at where exists (select 1 from dedup) and dw.customer.version < excluded.version; -- reject stale Watermarks: if your source can deliver events very late, set a watermark ( \u201cignore events older than 24h unless in backfill mode\u201d) and expose a replay switch for operators. Acceptance tests Black-box checks you can run in the lab: -- 1) Per-key order is preserved across replays -- Produce events e(v=1), e(v=2) for id=A; consume twice; sink result must reflect v=2. -- 2) Duplicate with same version is ignored (idempotent) -- Apply e(v=2) twice; final state remains v=2. -- 3) Late arrival is rejected by version guard -- Apply e(v=3), then e(v=2); final state remains v=3. -- 4) Backfill mode temporarily disables watermark, but version guard still holds -- Replays that are older than watermark do not revert newer versions. FAQ \u2014 partitioning & reconciliation What\u2019s a good default key? Use a stable business key that maps one entity to one partition (customer_id ). If multi-tenant, prefix with tenant_id or shard hot tenants. How many partitions? Enough so peak per-partition rate stays below consumer capacity with headroom (p95 < 60\u201370% of capacity). Re-evaluate after observing real traffic. Do I need global order? Rarely. Design for per-entity correctness. If you truly need cross-entity order, you\u2019re in batch territory or need a single partition (at significant cost). \u2190 Back: Multi-Tenancy Next: Hands-On Lab \u2192 \u00a9 2025 Christopher Ennis. A deep dive into the world of Change Data Capture."
  },
  {
    "path": "quickstarts.html",
    "title": "quickstarts \u2014 postgres \u2022 mysql \u2022 oracle",
    "text": "quickstarts \u2014 postgres \u2022 mysql \u2022 oracle \u2190 back to overview quickstarts choose your source and follow a short, opinionated runbook. each includes prerequisites, config, verification, and safe rollback. \ud83d\udc18 postgres enable logical decoding, register Debezium, watch change events end-to-end. wal_level=logical , slots allowed PKs or REPLICA IDENTITY FULL open postgres quickstart ~10\u201315 min \ud83d\udc2c mysql binlog in row mode, narrow includes, confirm topics receive events. binlog_format=ROW gtid_mode=ON preferred open mysql quickstart ~10\u201315 min \ud83d\udfe0 oracle confirm ARCHIVELOG + supplemental logging, align connector, and verify redo flow. ARCHIVELOG mode DB + table supplemental logging open oracle quickstart ~15\u201320 min acceptance tests first 15 minutes oracle notes hands-on lab"
  },
  {
    "path": "tests.html",
    "title": "acceptance tests \u2014 kafka + debezium lab",
    "text": "acceptance tests \u2014 kafka + debezium lab \u2190 back to lab acceptance tests \u2014 kafka + debezium lab these tests run on your machine to prove the lab works: containers up, connector healthy, topic has change events, and (optionally) offsets advance across a restart. what you\u2019ll run scripts/test_stack.sh \u2014 zookeeper, broker, connect, pg are running & connect REST is reachable scripts/test_connector.sh \u2014 connector exists and is RUNNING scripts/test_events.sh \u2014 topic has data; consumed messages parse as JSON and include op \u2208 {c,u,d} scripts/test_chaos_smoke.sh (optional) \u2014 latest offsets increase after a controlled connector restart requirements: docker , docker compose , curl , jq . windows users: run under WSL or Git Bash. run them (mac/linux + wsl) chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh # optional: bash scripts/test_chaos_smoke.sh expect green checks (\u2705). any \u274c includes the failing step so you know what to fix. windows (powershell via wsl) open \u201cubuntu (wsl)\u201d cd into the lab folder you mounted (/mnt/c/Users/you/lab ) run the same bash commands as left native powershell can work if you have Git Bash; otherwise use WSL. customizing (if your names differ) override env vars inline: CONNECT_URL=http://connect:8083 \\ CONNECTOR_NAME=my-connector \\ TOPIC=my.db.my_table \\ bash scripts/test_events.sh defaults are CONNECT_URL=http://localhost:8083 , CONNECTOR_NAME=inventory-connector , TOPIC=server1.public.app_customer . what \u201cpass\u201d means stack : all containers are up and connect\u2019s REST API responds connector : the named source connector is RUNNING with at least one RUNNING task events : the Debezium topic has data and you can parse JSON records with op=c/u/d chaos smoke : after a connector restart, the topic\u2019s latest offset increases (someone is producing / the source didn\u2019t wedge) troubleshooting quickies no messages consumed : generate a change in app.customer (insert/update/delete), then re-run test_events.sh connector not found : register it (see lab step \u201cregister the connector\u201d), or set CONNECTOR_NAME timeouts : ensure docker is running; docker compose ps should show 4 services up permission denied : run chmod +x scripts/*.sh once"
  },
  {
    "path": "troubleshooting.html",
    "title": "first 15 minutes \u2014 cdc troubleshooting",
    "text": "first 15 minutes \u2014 cdc troubleshooting \u2190 back to home first 15 minutes: cdc troubleshooting a repeatable triage to stabilize incidents fast. keep it pragmatic, measurable, and reversible. quick triage (5\u20138 minutes) freeze changes that make state drift: pause backfills, schema changes, and connector restarts. define the failure mode : stuck (no progress), slow (lag growing), wrong (duplicates, missing rows), or crashing . bound the blast radius : single table/tenant vs global; snapshot vs stream. checkpoint evidence (timestamps, connector name, offsets, group id). don\u2019t tail logs without writing down the top lines you see. choose a safety : if logs are at risk of rotating out, extend retention (WAL/binlog/redo) before touching the pipeline. goal: stop data loss, capture proof, buy time. detailed checks below. artifacts to collect (copy/paste) versions: source db, connector (Debezium), broker (Kafka/Redpanda), sink connector config (sanitized): include snapshot mode, includes/excludes, heartbeat exact error lines: 20\u201350 lines around the first failure lag/offsets: consumer group lag and last committed LSN/GTID/SCN log retention settings: WAL/binlog/redo retention and current oldest log commands kafka (consumer lag) copy kafka-consumer-groups --bootstrap-server <host:port> \\ --describe --group <your-sink-group> connect (connector status) copy curl -s http://<connect-host:8083>/connectors | jq curl -s http://<connect-host:8083>/connectors/<name>/status | jq stabilize first ensure source logs retained \u2265 time to fix + snapshot duration enable DLQ (or equivalent) to prevent silent drops reduce parallelism if source is choking (snapshot & stream task counts) if duplicates are appearing, convert sink writes to idempotent upserts (MERGE/UPSERT) immediately postgres quick checks configuration + health copy -- WAL & slots SHOW wal_level; -- should be 'logical' SHOW max_wal_senders; -- >= number of replication clients SHOW max_replication_slots; -- >= slots in use SELECT slot_name, active, restart_lsn, confirmed_flush_lsn FROM pg_replication_slots; -- retention & pressure SHOW wal_keep_size; -- size hint for retained WAL SELECT now() - pg_last_wal_replay_lsn()::text::pg_lsn; -- on standby -- lag (if using logical slot) SELECT pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS retained_from_slot FROM pg_replication_slots WHERE slot_name='<slot>'; common symptoms \u2192 first aid snapshot stalls: reduce fetch size; whitelist fewer tables; ensure long-running txns aren\u2019t blocking. slot disk pressure: sink caught behind \u2192 raise sink throughput or pause snapshot; never drop the slot without a plan. missing updates: replica identity not full on keyless tables \u2192 set REPLICA IDENTITY FULL where needed. mysql quick checks configuration + health copy -- binlog mode SHOW VARIABLES LIKE 'binlog_format'; -- should be ROW SHOW VARIABLES LIKE 'binlog_row_image'; -- FULL or MINIMAL (know your connector's needs) SHOW VARIABLES LIKE 'gtid_mode'; -- ON preferred SHOW MASTER STATUS; -- file/pos + executed GTID set -- retention (8.0+) SHOW VARIABLES LIKE 'binlog_expire_logs_seconds'; -- heartbeat/throughput SHOW GLOBAL STATUS LIKE 'Binlog_cache_disk_use'; common symptoms \u2192 first aid duplicates after restart: sink not idempotent \u2192 change inserts to MERGE/UPSERT with stable pk + version. lost history: binlogs expired during snapshot \u2192 extend expire window; restart from fresh snapshot. DDL breakage: add include.schema.changes (if supported) and verify sink DDL policy. oracle quick checks configuration + health copy -- supplemental logging SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; -- database log mode SELECT log_mode FROM v$database; -- ARCHIVELOG recommended for CDC -- redo switch & archive status SELECT sequence#, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 5 ROWS ONLY; -- confirm key columns logging for keyless tables (optional) SELECT * FROM dba_log_groups WHERE LOG_GROUP_TYPE IN ('ALL COLUMN LOGGING','PRIMARY KEY LOGGING'); common symptoms \u2192 first aid high redo churn: throttle snapshot/table set; ensure filters aren\u2019t too broad. missed updates: missing supplemental logging for key columns \u2192 add minimal or table-level logging, resnapshot. catalog mode mismatch after upgrade: re-check connector\u2019s catalog strategy defaults before restart. kafka / connect quick checks status & lag copy # list & inspect curl -s http://<connect:8083>/connectors | jq curl -s http://<connect:8083>/connectors/<name>/status | jq # consumer lag (sink) kafka-consumer-groups --bootstrap-server <host:port> \\ --describe --group <your-sink-group> # safe restart of a sick task curl -s -XPOST http://<connect:8083>/connectors/<name>/tasks/0/restart common symptoms \u2192 first aid connector crash loops: identify first error, enable DLQ ( errors.tolerance=all + DLQ topic), then fix offending table. slow ingestion: increase tasks up to source limits; ensure topic partitions \u2265 parallelism; watch sink bottlenecks first. out-of-order within key: ensure producer uses key-aware partitioner; keep per-key ordering on the sink merge path. sink verification (duplicates / missing rows) duplicate primary keys copy -- generic template (replace table/pk) SELECT COUNT(*) AS rows, COUNT(DISTINCT pk) AS distinct_keys FROM target_table; latest-wins check copy -- per business key, do we keep the latest op_ts/version? SELECT key_col, MAX(op_ts) AS last_ts, COUNT(*) AS events FROM staging_or_history GROUP BY key_col HAVING COUNT(*) <> 1 AND MAX(op_ts) < NOW() - INTERVAL '0 seconds'; if duplicates exist, convert the sink write to an idempotent MERGE keyed on a stable id + version/op_ts and re-run the last N events. when to escalate source log gap detected (WAL/binlog/redo missing) and you can\u2019t reconstruct from another source \u2192 plan a clean resnapshot. schema/key change incompatible with current sink merge logic \u2192 schedule maintenance window for transform + backfill. security/privileges prevent enabling required logging \u2192 involve dbas (don\u2019t keep retrying the connector). acceptance for \u201cstabilized\u201d consumer lag is flat or shrinking for 30 minutes latest offsets/LSN/GTID/SCN are advancing DLQ is empty or only contains triaged, expected errors duplicate-PK query shows rows == distinct_keys a controlled connector restart does not create new duplicates"
  },
  {
    "path": "quickstart/quickstart-mysql.html",
    "title": "mysql CDC \u2014 quickstart & runbook",
    "text": "mysql CDC \u2014 quickstart & runbook \u2190 home mysql CDC \u2014 quickstart & runbook configure binlog safely, register a connector, verify changes, and roll back cleanly. prerequisites (db) binlog_format=row , binlog_row_image FULL or MINIMAL per connector support GTID mode preferred ( gtid_mode=ON ) binlog retention window \u2265 snapshot + catch-up time stable primary keys (or sink logic that can reconcile keyless tables) SHOW VARIABLES LIKE 'binlog_format'; SHOW VARIABLES LIKE 'binlog_row_image'; SHOW VARIABLES LIKE 'gtid_mode'; SHOW MASTER STATUS; SHOW VARIABLES LIKE 'binlog_expire_logs_seconds'; DB setup (copy/paste) -- my.cnf (server) binlog_format = ROW binlog_row_image = FULL server_id = 1 gtid_mode = ON enforce_gtid_consistency = ON -- grant minimal privileges to CDC user GRANT REPLICATION SLAVE, REPLICATION CLIENT, SELECT ON *.* TO 'cdc'@'%' IDENTIFIED BY 'cdc'; connector config (example) { \"name\": \"mysql-cdc\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"cdc\", \"database.password\": \"cdc\", \"database.server.id\": \"5400\", \"topic.prefix\": \"server1\", \"database.include.list\": \"app\", \"table.include.list\": \"app.customer\", \"tombstones.on.delete\": \"false\", \"include.schema.changes\": \"false\", \"snapshot.mode\": \"initial\", \"errors.tolerance\": \"all\", \"errors.deadletterqueue.topic.name\": \"dlq.mysql\" } } verify -- generate a change UPDATE app.customer SET email = CONCAT(email,'.x') WHERE id = (SELECT id FROM app.customer LIMIT 1); -- confirm binlog advancing SHOW MASTER STATUS; -- topic should receive a record for app.customer kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic server1.app.customer --from-beginning --max-messages 5 acceptance (target/sink) no duplicate PKs after connector restart latest-wins per key using op_ts/version SELECT COUNT(*) rows, COUNT(DISTINCT id) distinct_keys FROM target_customers; safe rollback stop the connector; keep binlogs until you\u2019re sure downstream is consistent. if you must prune, ensure all consumers caught up."
  },
  {
    "path": "quickstart/quickstart-oracle.html",
    "title": "oracle CDC \u2014 quickstart & runbook",
    "text": "oracle CDC \u2014 quickstart & runbook \u2190 home oracle CDC \u2014 quickstart & runbook confirm prerequisites, collect health signals, register connector, verify changes, and roll back safely. prerequisites (db) ARCHIVELOG mode recommended supplemental logging enabled (DB-level minimal; table-level for key columns or ALL for keyless) redo/archive retention window \u2265 snapshot + catch-up stable primary keys (or table log groups capturing merge keys) -- archivelog and redo SELECT log_mode FROM v$database; SELECT sequence#, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 5 ROWS ONLY; -- supplemental logging SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; SELECT owner, table_name, log_group_name, always, log_group_type FROM dba_log_groups ORDER BY owner, table_name; DB setup (examples) -- table-level log group for keyless merge ALTER TABLE APP.CUSTOMER ADD LOG GROUP lg_customer_pk (ID) ALWAYS; -- throttle snapshot scope to avoid redo storms (choose limited tables) -- and plan off-hours if tables are huge connector notes prefer narrow includes/filters to reduce LogMiner workload measure redo switch rate before/after snapshot have a plan for LOBs (exclude or route separately if large) { \"name\": \"oracle-cdc\", \"config\": { \"connector.class\": \"io.debezium.connector.oracle.OracleConnector\", \"database.hostname\": \"oracle\", \"database.port\": \"1521\", \"database.user\": \"CDC\", \"database.password\": \"CDC\", \"database.dbname\": \"ORCLCDB\", \"database.pdb.name\": \"ORCLPDB1\", \"topic.prefix\": \"server1\", \"schema.include.list\": \"APP\", \"table.include.list\": \"APP.CUSTOMER\", \"tombstones.on.delete\": \"false\", \"include.schema.changes\": \"false\", \"snapshot.mode\": \"initial\", \"errors.tolerance\": \"all\", \"errors.deadletterqueue.topic.name\": \"dlq.oracle\" } } verify -- generate a change UPDATE APP.CUSTOMER SET EMAIL = EMAIL || '.X' WHERE ROWNUM = 1; -- redo activity sample SELECT TO_CHAR(first_time,'YYYY-MM-DD HH24:MI') t, COUNT(*) switches FROM v$log_history WHERE first_time > SYSDATE - 1/24 GROUP BY TO_CHAR(first_time,'YYYY-MM-DD HH24:MI'); kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic server1.APP.CUSTOMER --from-beginning --max-messages 5 acceptance (target/sink) no duplicate PKs after connector restart latest-wins per key by commit timestamp/SCN DLQ empty or only expected test errors SELECT COUNT(*) rows, COUNT(DISTINCT ID) distinct_keys FROM TARGET_CUSTOMERS; safe rollback pause connector; keep archived logs until downstream is consistent. if schema/log-group changes are needed, plan a clean re-snapshot of affected tables."
  },
  {
    "path": "quickstart/quickstart-postgres.html",
    "title": "postgres CDC \u2014 quickstart & runbook",
    "text": "postgres CDC \u2014 quickstart & runbook \u2190 home postgres CDC \u2014 quickstart & runbook set the right DB knobs, register a connector, verify changes, and know how to roll back safely. prerequisites (db) wal_level=logical , slots allowed, publication permissions tables have a stable primary key (or set REPLICA IDENTITY FULL for keyless) WAL retention window \u2265 snapshot + catch-up time SHOW wal_level; SHOW max_replication_slots; SHOW max_wal_senders; SELECT current_setting('wal_keep_size'); DB setup (copy/paste) -- minimal role for logical replication (adjust as needed) CREATE ROLE cdc LOGIN PASSWORD 'cdc' REPLICATION; -- ensure replica identity for keyless tables (example) ALTER TABLE app.customer REPLICA IDENTITY FULL; -- create publication if you prefer manual management CREATE PUBLICATION cdc_pub FOR TABLE app.customer; if using Debezium, you can let it auto-create a filtered publication. connector config (example) { \"name\": \"pg-cdc\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"database.hostname\": \"pg\", \"database.port\": \"5432\", \"database.user\": \"cdc\", \"database.password\": \"cdc\", \"database.dbname\": \"postgres\", \"slot.name\": \"cdc_slot\", \"publication.autocreate.mode\": \"filtered\", \"schema.include.list\": \"app\", \"table.include.list\": \"app.customer\", \"tombstones.on.delete\": \"false\", \"heartbeat.interval.ms\": \"5000\", \"include.schema.changes\": \"false\", \"decimal.handling.mode\": \"string\", \"snapshot.mode\": \"initial\", \"errors.tolerance\": \"all\", \"errors.deadletterqueue.topic.name\": \"dlq.pg\" } } verify -- slot & publication health SELECT slot_name, active, restart_lsn, confirmed_flush_lsn FROM pg_replication_slots WHERE slot_name='cdc_slot'; SELECT * FROM pg_publication_tables WHERE pubname='cdc_pub'; -- generate a change UPDATE app.customer SET email = email || '.x' WHERE id = (SELECT id FROM app.customer LIMIT 1); # consume from the topic (adjust name) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic server1.public.app_customer --from-beginning --max-messages 5 acceptance (target/sink) no duplicate PKs after a connector restart per key, the row reflects the greatest op_ts/version DLQ empty or only expected test errors -- duplicates (generic) SELECT COUNT(*) rows, COUNT(DISTINCT id) distinct_keys FROM target_customers; safe rollback -- stop connector first (to avoid slot churn) -- then, if you must remove the slot: SELECT pg_drop_replication_slot('cdc_slot'); dropping a live slot can force WAL recycling and data loss for downstream readers. pause, snapshot plan, then drop."
  }
]
