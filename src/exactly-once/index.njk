---
layout: base.njk
title: "Exactly-Once Semantics"
description: "A deep dive into at-least-once vs. exactly-once semantics, idempotency, and the transactional outbox pattern for reliable data pipelines."
canonicalPath: "/exactly-once/"
head_extra: |
  <link rel="stylesheet" href="{{ '/assets/css/pages/exactly-once.css' | url }}">
  <script type="application/ld+json">
        {
          "@context": "https://schema.org",
          "@type": "TechArticle",
          "headline": "Exactly-Once Semantics",
          "description": "A deep dive into at-least-once vs. exactly-once semantics, idempotency, and the transactional outbox pattern for reliable data pipelines.",
          "url": "https://letstalkcdc.nfshost.com/exactly-once/",
          "inLanguage": "en",
          "about": [
            "Change Data Capture",
            "Exactly-Once Semantics",
            "Idempotency",
            "Transactional Outbox",
            "Data Engineering"
          ]
        }
      </script>
  <script type="application/ld+json">
        {
          "@context": "https://schema.org",
          "@type": "BreadcrumbList",
          "itemListElement": [
            {
              "@type": "ListItem",
              "position": 1,
              "name": "Series Overview",
              "item": "https://letstalkcdc.nfshost.com/overview/"
            },
            {
              "@type": "ListItem",
              "position": 2,
              "name": "Exactly-Once Semantics",
              "item": "https://letstalkcdc.nfshost.com/exactly-once/"
            }
          ]
        }
      </script>
scripts:
  - "/assets/js/pages/exactly-once.js"

---
{% import "components/ui.njk" as ui %}

{{ ui.hero(heroConfig) | safe }}

<div class="page-wrap prose">
      <article class="prose">
        <p>
          End-to-end "exactly-once <em>delivery</em>" across <em>heterogeneous</em> systems
          (DB ‚Üí Kafka ‚Üí warehouse/search) is not achievable in general. What we actually ship is
          <strong>Exactly-Once <em>Processing</em> (EOP)</strong>: combine <strong>At-Least-Once (ALO)</strong>
          transport with <strong>idempotent</strong> sinks so replays don‚Äôt create duplicates.
        </p>
        <p>
          Scope matters:
          <ul>
            <li><strong>Source connectors (e.g., Debezium):</strong> ALO into Kafka. They can‚Äôt make the
              external database ‚Äútransactional‚Äù with Kafka.</li>
            <li><strong>Within Kafka‚ÜíKafka topologies:</strong> EOS is possible using Kafka transactions
              (idempotent producers + transactional offsets), but the scope is limited to Kafka topics.</li>
            <li><strong>Kafka ‚Üí external sinks:</strong> Effective EOS comes from <em>idempotent upserts</em>
              (or staging+MERGE) at the destination, not from transport guarantees alone.</li>
          </ul>
        </p>
        <p>
          How a CDC pipeline behaves during failures is defined by its data
          delivery guarantees:
        </p>
        <ul>
          <li>
            <strong>At-Most-Once:</strong> The weakest guarantee. A message is
            sent once, but if a failure occurs before it's processed, the
            message is lost forever. Unsuitable for critical data.
          </li>
          <li>
            <strong>At-Least-Once:</strong> The most common default. The system
            ensures every message is delivered, but a message might be delivered
            more than once during a retry. This prevents data loss but requires
            consumers to handle duplicates.
          </li>
          <li>
            <strong>Exactly-Once Processing:</strong> The effective goal. The
            system delivers every message at least once, and the consumer is
            designed to process each unique message precisely one time, even if
            it receives duplicates.
          </li>
        </ul>
        <h2 id="at-least-once">The Problem: Duplicates in At-Least-Once Systems</h2>
        <p>
          Imagine a consumer reads an event from a Kafka topic, processes it
          (writes to a database), and then crashes before it can commit the
          topic offset. When the consumer restarts, it will re-read the same
          event, leading to duplicate processing. The interactive demo below
          walks through this failure scenario.
        </p>
      </article>

      <div class="interactive-demo" id="demo">
        <div class="step-diagram">
          <div class="step" id="step-1">
            <h4>1. Consume Message</h4>
            <p>
              The consumer service pulls a message (`Order #123`) from the
              message broker.
            </p>
          </div>
          <div class="step" id="step-2">
            <h4>2. Process &amp; Write to Sink</h4>
            <p>
              The service processes the order and successfully inserts a record
              into the `orders` table in the downstream database.
            </p>
          </div>
          <div class="step" id="step-3">
            <h4>3. CRASH! üí•</h4>
            <p>
              Before the service can acknowledge the message by committing its
              offset to the broker, the service crashes. The broker assumes the
              message was never processed.
            </p>
          </div>
          <div class="step" id="step-4">
            <h4>4. Restart &amp; Re-process</h4>
            <p>
              The service restarts. Since the offset was not committed, it
              fetches the same message (`Order #123`) again and inserts a
              <em>duplicate record</em> into the `orders` table.
            </p>
          </div>
        </div>
        <div class="diagram-visual-panel">
          <div
            class="w-full text-center transition-all duration-500"
            id="diagram-visual"
          >
            <div class="icon">üé¨</div>
            <p class="mt-2 font-semibold">Ready to start</p>
          </div>
          <div style="display:grid; gap:.5rem; width:100%; margin-top:1rem">
            <button class="button" id="failure-stepper">Start Failure</button>
            <button class="button" id="idempotent-stepper" aria-label="Show idempotent handling">
              Show Idempotent Handling
            </button>
          </div>
        </div>
      </div>

      <article class="prose">
        <h2 id="idempotency">The Solution: Idempotent Consumers</h2>
        <p>
          <strong>Idempotency</strong> is the property of an operation that
          allows it to be applied multiple times without changing the result
          beyond the initial application. An idempotent consumer can safely
          process the same message multiple times with no side effects. This
          requires two things:
        </p>
        <ol>
          <li>
            <strong>A Unique, Deterministic Event ID:</strong> Every change
            event must have a unique identifier. This can be a UUID from an
            outbox table or a composite key of the source table's primary key
            and the transaction's log position.
          </li>
          <li>
            <strong>Deduplication Logic at the Sink:</strong> The consumer uses
            this ID to check if the event has already been processed before
            applying the change. Common patterns include:
            <ul>
              <li>
                <strong>Using `MERGE` or `UPSERT`:</strong> The sink database
                handles the logic of creating a new record or updating an
                existing one atomically. This is the preferred method.
              </li>
              <li>
                <strong>Using a Deduplication Table:</strong> The consumer first
                tries to `INSERT` the event ID into a `processed_events` table.
                If it succeeds, it proceeds; if it fails (due to a primary key
                violation), it knows the event is a duplicate and can be safely
                ignored.
              </li>
              <li>
                <strong>Staging + MERGE:</strong> Land events into a staging table keyed by
                <code>event_id</code>, then <code>MERGE</code> into the target to make replays safe.
              </li>
            </ul>
          </li>
        </ol>
        <p><em>Commit order:</em> write to sink ‚Üí then commit offsets. If offsets commit first,
        a crash can drop data; idempotent sinks tolerate replays after restart.</p>
        <div class="code-comparison">
          <div>
            <h4>Naive Consumer</h4>
            <pre><code>// Pseudocode: may create duplicates
for (event of stream) {
  // Simple insert will create a new
  // record for every retry.
  insert_into_sink(event.payload);
  commit_offset();
}</code></pre>
          </div>
          <div>
            <h4>Idempotent Consumer</h4>
            <pre><code>// Pseudocode: safe from duplicates
for (event of stream) {
  // MERGE/UPSERT handles existing records
  // based on a primary key.
  upsert_into_sink(
    event.key,
    event.payload
  );
  commit_offset();
}</code></pre>
          </div>
        </div>
        <div class="code-comparison">
          <div>
            <h4>Warehouse MERGE (Snowflake-style)</h4>
<pre><code>MERGE INTO dw.orders AS t
USING (SELECT :event_id AS event_id, :order_id AS order_id, :payload::variant AS p) s
ON t.event_id = s.event_id
WHEN NOT MATCHED THEN
  INSERT (event_id, order_id, payload) VALUES (s.event_id, s.order_id, s.p)
WHEN MATCHED THEN
  UPDATE SET payload = s.p;  -- idempotent overwrite
</code></pre>
          </div>
          <div>
            <h4>BigQuery MERGE (idempotent)</h4>
<pre><code>MERGE `dw.orders` t
USING (SELECT @event_id AS event_id, @order_id AS order_id, @payload AS payload) s
ON t.event_id = s.event_id
WHEN NOT MATCHED THEN
  INSERT (event_id, order_id, payload) VALUES (s.event_id, s.order_id, s.payload)
WHEN MATCHED THEN
  UPDATE SET payload = s.payload;
</code></pre>
          </div>
        </div>
        <h2 id="outbox">
          The Transactional Outbox Pattern (Solving the Dual-Write Problem)
        </h2>
        <p>
          A common anti-pattern is the "dual-write," where an application first
          writes to its database and *then*, in a separate network call, tries
          to publish a message. If the application crashes between these two
          steps, the systems become inconsistent. The
          <strong>Transactional Outbox Pattern</strong> solves this.
        </p>
        <ol>
          <li>
            An application writes both its business data (an `orders` record)
            and an event record to an "outbox" table within the same single,
            atomic database transaction.
          </li>
          <li>
            A log-based CDC process monitors the outbox table. When it detects
            the committed transaction in the database log, it reliably reads the
            event from the log and publishes it to a message bus like Kafka.
          </li>
          <li>
            This guarantees that an event is published if, and only if, the
            corresponding business transaction was successfully committed to the
            database. Data consistency is preserved.
          </li>
        </ol>
        <p>
          <strong>Important scope note:</strong> Outbox gives you EOS between the application DB and Kafka
          (no lost/phantom messages). It does <em>not</em> make your downstream warehouses exactly-once by itself‚Äî
          you still need idempotent consumers at the sinks.
        </p>
      </article>
</div>
