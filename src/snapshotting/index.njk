---
layout: base.njk
title: "Snapshotting (Initial Load)"
description: "Learn how to handle the initial data load (snapshotting) in a CDC pipeline consistently and without data loss."
canonicalPath: "/snapshotting/"
head_extra: |
  <link rel="stylesheet" href="/assets/css/pages/snapshotting.css">
  <!-- SME STYLE START -->
  
  <!-- SME STYLE END -->
  <!-- SME SCRIPT START -->
  <!-- SME SCRIPT END -->
  <!-- SME DARK CALLOUT START -->
  
  <!-- SME DARK CALLOUT END -->
  <!-- SME THEME FIXES START -->
  
  <!-- SME THEME FIXES END -->
  <!-- SME SNAPSHOT THEME OVERRIDE START -->
  
  <!-- SME SNAPSHOT THEME OVERRIDE END -->

---
{% import "components/ui.njk" as ui %}

{{ ui.hero(heroConfig) | safe }}

<div class="page-wrap prose">

  <nav class="intro-quick-nav" aria-label="Intro navigation">
    <ul>
      <li><a href="#playbook">Playbook overview</a></li>
      <li><a href="#preflight">Checklist</a></li>
      <li><a href="#challenge">Concept</a></li>
      <li><a href="#perdb">DB recipes</a></li>
    </ul>
  </nav>

  <section id="playbook" aria-labelledby="playbook-title" class="intro-playbook">
    <div class="intro-lede">
      <h2 id="playbook-title">Start with a clean handoff</h2>
      <p>
        <strong>80% of CDC escalations originate during the initial load.</strong>
        Teams rush the boundary, purge logs, and spend weeks replaying tables.
      </p>
      <p>
        Use these three moves to align engineering, operations, and analytics before the stream goes live.
      </p>
      <a class="intro-lab-link" href="#lab">Prove it in 15 minutes →</a>
    </div>

    <section class="handoff" aria-labelledby="handoff-h2">
      <h3 id="handoff-h2" class="sr-only">High-watermark boundary handoff</h3>
      <ol class="handoff-steps" role="list">
        <li class="stage">
          <span class="num" aria-hidden="true">1</span>
          <a class="stage-body" href="#boundary">
            <span class="stage-title">Mark high-watermark</span>
            <span class="stage-sub">Record LSN/SCN boundary</span>
          </a>
        </li>
        <li class="arrow" aria-hidden="true">→</li>
        <li class="stage">
          <span class="num" aria-hidden="true">2</span>
          <a class="stage-body" href="#snapread">
            <span class="stage-title">Consistent read</span>
            <span class="stage-sub">Snapshot without gaps</span>
          </a>
        </li>
        <li class="arrow" aria-hidden="true">→</li>
        <li class="stage">
          <span class="num" aria-hidden="true">3</span>
          <a class="stage-body" href="#streamhandoff">
            <span class="stage-title">Stream after boundary</span>
            <span class="stage-sub">Apply idempotently</span>
          </a>
        </li>
      </ol>
      <p class="handoff-cap">
        Snapshot → high-watermark → stream. Preserve per-key order and use idempotent upserts/deletes at the sink.
      </p>
    </section>

    <ul class="learn-grid">
      <li>
        <span class="learn-icon" aria-hidden="true">①</span>
        <div>
          <strong>Boundary & retention.</strong>
          <p>Set the log high-watermark, confirm retention covers the run, and name an owner for recovery points.</p>
        </div>
      </li>
      <li>
        <span class="learn-icon" aria-hidden="true">②</span>
        <div>
          <strong>Execution made safe.</strong>
          <p>Pick the right snapshot mode, chunk large tables, and cap parallelism to respect OLTP workloads.</p>
        </div>
      </li>
      <li>
        <span class="learn-icon" aria-hidden="true">③</span>
        <div>
          <strong>Proof & recovery.</strong>
          <p>Run the quick lab, monitor offsets and lag, and document restart checkpoints.</p>
        </div>
      </li>
    </ul>
  </section>

  <section id="preflight" aria-labelledby="preflight-title" class="preflight-card">
    <h2 id="preflight-title">Pre-flight checklist</h2>
    <p class="preflight-blurb">Confirm these guardrails before the snapshot starts rolling.</p>
    <details class="preflight-details">
      <summary>View checklist</summary>
      <ul>
        <li>
          Retention window covers full snapshot duration (WAL/binlog/T-log/UNDO).
        </li>
        <li>Privileges for log reading/replication are granted.</li>
        <li>
          Consistent-read isolation is enabled where applicable (REPEATABLE READ/SNAPSHOT).
        </li>
        <li>
          Sink MERGE/UPSERT implemented; delete/tombstone semantics defined.
        </li>
        <li>Offsets are durable and backed up/versioned.</li>
        <li>DDL freeze or schema-compatibility policy in place.</li>
      </ul>
    </details>
  </section>

  <section id="compare" aria-labelledby="compare-title" class="intro-compare">
    <h2 id="compare-title">Why the boundary wins</h2>
    <table class="compare-table">
      <thead>
        <tr>
          <th scope="col">Run style</th>
          <th scope="col">What happens mid-snapshot</th>
          <th scope="col">Recovery story</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th scope="row">Naive initial load</th>
          <td>Changes land twice or not at all; log position is unknown.</td>
          <td>Full re-run of large tables; risky ad-hoc dedupe scripts.</td>
        </tr>
        <tr>
          <th scope="row">Snapshot with boundary</th>
          <td>Log handoff preserves ordering; replays apply idempotently.</td>
          <td>Resume from last chunk + saved offset; no target surgery.</td>
        </tr>
        <tr>
          <th scope="row">Incremental snapshot</th>
          <td>Primary-key ranges throttle load while respecting OLTP.</td>
          <td>Retries pick up the next range; log anchor stays valid.</td>
        </tr>
      </tbody>
    </table>
  </section>

  <article class="prose">
        <p>
          A CDC pipeline is designed to stream ongoing <em>changes</em>, but it
          must first be initialized with the data that
          <em>already exists</em> in the source tables. This process of
          performing an initial, full copy of the data is known as
          <strong>snapshotting</strong> or the <strong>initial load</strong>.
        </p>

        <h2 id="challenge">Why snapshots break</h2>
        <p>
          The central problem is consistency. Without a boundary, writes that
          land mid-run show up twice (snapshot + stream) or disappear entirely.
          Retention clocks keep ticking and, if the log purges, the only fix is
          to start over.
        </p>
        <p>
          Picture a payments table: you start scanning, finance posts refunds,
          and analytics reconciles against a different truth hours later. The
          fallout is expensive clean-up, tense incident calls, and a loss of
          trust in the pipeline.
        </p>

        <h2 id="solution">A staged handoff</h2>
        <p>
          Modern log-based CDC tools coordinate a
          <em>point-in-time snapshot</em> with the transaction log so the
          handoff to streaming has no gaps or double-counts. Treat the run in
          three phases:
        </p>

        <div class="handoff-timeline" role="list">
          <article class="timeline-item" id="boundary" role="listitem">
            <h3>Before snapshot: mark the high-watermark</h3>
            <p>
              Read and persist the current log position (LSN/SCN). Share it in
              the runbook and verify retention outlives the planned duration.
            </p>
            <ul>
              <li>Capture boundary metadata alongside connector configuration.</li>
              <li>Alert on log growth so retention doesn’t expire mid-run.</li>
            </ul>
          </article>

          <article class="timeline-item" id="snapread" role="listitem">
            <h3>During snapshot: hold a consistent read</h3>
            <p>
              Use snapshot/consistent-read isolation and scan tables in
              deterministic chunks. The goal is predictable load that won’t pin
              OLTP locks or exhaust undo segments.
            </p>
            <ul>
              <li>Chunk big tables in primary-key order and cap parallelism.</li>
              <li>Read from a replica when possible; monitor lag to stay honest.</li>
            </ul>
          </article>

          <article class="timeline-item" id="streamhandoff" role="listitem">
            <h3>After snapshot: stream from the boundary</h3>
            <p>
              Start consuming the log from the recorded position. Any rows that
              change post-snapshot must apply idempotently so retries are safe.
            </p>
            <ul>
              <li>Implement UPSERT/MERGE semantics and explicit delete handling.</li>
              <li>Version and store offsets so restart is a click, not a rebuild.</li>
            </ul>
          </article>
        </div>

        <section id="perdb" aria-labelledby="perdb-title" class="perdb-recipes">
          <h2 id="perdb-title">Run it on your database</h2>
          <p>
            Grab the snippet for your engine to capture the boundary and hold a
            consistent read while Debezium (or another connector) walks the
            tables.
          </p>

          <div class="perdb-grid">
            <details>
              <summary><strong>PostgreSQL</strong></summary>
              <pre><button class="copy-btn" type="button">Copy</button><code>-- Requirements (once/admin):
ALTER SYSTEM SET wal_level = logical;
ALTER SYSTEM SET max_replication_slots = 10;  -- adjust
SELECT pg_reload_conf();

-- Boundary + consistent read (within one tx while snapshotting):
BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
SELECT pg_current_wal_lsn();  -- record this
-- Keep transaction open during table scans; Debezium coordinates via logical slot.
</code></pre>
            </details>

            <details>
              <summary><strong>MySQL (InnoDB)</strong></summary>
              <pre><button class="copy-btn" type="button">Copy</button><code>-- Requirements:
-- binlog_format=ROW; binlog_row_image=FULL; GTID (preferred)

-- Boundary + consistent snapshot:
SHOW MASTER STATUS;                  -- or SELECT @@global.gtid_executed;
START TRANSACTION WITH CONSISTENT SNAPSHOT;
-- Read tables in PK order; Debezium avoids FTWRL in OLTP paths.
</code></pre>
            </details>

            <details>
              <summary><strong>SQL Server</strong></summary>
              <pre><button class="copy-btn" type="button">Copy</button><code>-- Requirements (once/admin):
ALTER DATABASE YourDb SET ALLOW_SNAPSHOT_ISOLATION ON;
ALTER DATABASE YourDb SET READ_COMMITTED_SNAPSHOT ON;
EXEC sys.sp_cdc_enable_db;  -- if using native CDC

-- Boundary + snapshot:
SELECT sys.fn_cdc_get_max_lsn();     -- record
SET TRANSACTION ISOLATION LEVEL SNAPSHOT;
BEGIN TRAN;
-- Read tables; keep tx open during snapshot.
</code></pre>
            </details>

            <details>
              <summary><strong>Oracle</strong></summary>
              <pre><button class="copy-btn" type="button">Copy</button><code>-- Requirements (once/admin):
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;  -- minimal or ALL columns for PK-less

-- Boundary:
SELECT CURRENT_SCN FROM V$DATABASE;        -- record
-- Consistent reads use UNDO/flashback; ensure UNDO retention covers snapshot.
</code></pre>
            </details>
          </div>

          <details class="perdb-cheatsheet">
            <summary><strong>How consistent reads behave</strong></summary>
            <ul>
              <li>
                <strong>PostgreSQL:</strong> REPEATABLE READ +
                <code>pg_export_snapshot()</code> keeps scans stable; long scans
                can delay VACUUM, so watch table bloat alerts.
              </li>
              <li>
                <strong>MySQL/InnoDB:</strong>
                <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code> anchors
                to a binlog position—no <code>FLUSH TABLES WITH READ LOCK</code>
                required for OLTP safety.
              </li>
              <li>
                <strong>SQL Server:</strong> CDC tables + max LSN establish the
                boundary; snapshot isolation or read committed snapshot keeps
                readers from blocking writers.
              </li>
              <li>
                <strong>Oracle:</strong> A saved SCN plus UNDO retention gives
                you flashback reads; make sure UNDO window covers the run.
              </li>
            </ul>
          </details>
        </section>

        <h2 id="modes">Snapshot modes (choose intentionally)</h2>
        <ul>
          <li>
            <strong>initial:</strong> Do a snapshot then continue streaming
            (default for first run).
          </li>
          <li>
            <strong>initial_only:</strong> One-time bulk load, no streaming
            (bootstraps batch targets).
          </li>
          <li>
            <strong>schema_only / never:</strong> Skip data snapshot (for
            pre-seeded targets or special cases).
          </li>
          <li>
            <strong>incremental snapshot:</strong> Chunk tables online (no
            global pause) using low/high watermarks and primary-key ranges;
            supports resuming mid-table.
          </li>
        </ul>

        <h2 id="chunking">Performance: chunking &amp; throughput</h2>
        <ul>
          <li>
            <strong>Chunking:</strong> Read large tables in PK-ordered chunks
            (5k–200k rows) to bound memory and reduce long transactions. Prefer
            monotonic PKs; otherwise use synthetic chunking keys.
          </li>
          <li>
            <strong>Parallelism:</strong> Limit concurrent table snapshots to
            protect OLTP (1–3 in parallel).
          </li>
          <li>
            <strong>Throttling:</strong> Use fetch size / max rows per poll to
            keep steady pressure, not bursts.
          </li>
          <li>
            <strong>Replica reads:</strong> For MySQL/PG, consider reading from
            a replica to offload the primary (ensure replication lag is
            acceptable and log positions still align).
          </li>
        </ul>

        <h2 id="ops">Operational guardrails (day-one checks)</h2>
        <ul>
          <li>
            <strong>Log retention:</strong> Ensure WAL/binlog/T-log won’t be
            purged before snapshot finishes. Alert on backlog age and size.
          </li>
          <li>
            <strong>Privileges:</strong> Replication/log-read permissions;
            publication/slot creation where applicable.
          </li>
          <li>
            <strong>DDL during snapshot:</strong> Prefer compatibility rules
            (schema registry) and deploy windows; avoid table rebuilds that
            invalidate consistent reads.
          </li>
          <li>
            <strong>Sink idempotency:</strong> Upsert/delete semantics in the
            warehouse/search index must be idempotent so replays (post-snapshot)
            don’t duplicate data.
          </li>
          <li>
            <strong>Resume safely:</strong> Snapshots should be restartable:
            resume from last completed chunk and the recorded high-watermark.
          </li>
        </ul>
        <!-- SME ADDITIONS START: Patterns, Observability, Lab, Gotchas -->
        <section aria-labelledby="idempotent">
          <h2 id="idempotent">Idempotent sink patterns</h2>
          <p>Use UPSERT/MERGE semantics and explicit delete handling.</p>
          <details>
            <summary><strong>Warehouse MERGE example</strong></summary>
            <pre><button class="copy-btn" type="button">Copy</button><code>MERGE INTO dw.customers AS t
USING staging.customers AS s
ON t.customer_id = s.customer_id
WHEN MATCHED THEN UPDATE SET
  name = s.name,
  email = s.email,
  updated_at = s.updated_at
WHEN NOT MATCHED THEN INSERT (customer_id, name, email, updated_at)
VALUES (s.customer_id, s.name, s.email, s.updated_at);</code></pre>
          </details>
          <aside aria-label="Gotcha" class="callout">
            <strong>Gotcha:</strong> Handle deletes explicitly (tombstones or a
            <code>__op</code> field). Otherwise replays produce resurrected
            rows.
          </aside>
        </section>

        <section aria-labelledby="observability">
          <h2 id="observability">Observability: what to track</h2>
          <ul>
            <li>Source log lag (bytes/time behind) and slot/binlog growth.</li>
            <li>Snapshot throughput (rows/s), chunk duration, ETA by table.</li>
            <li>Sink merge latency and deadlocks.</li>
            <li>
              Error budget: % chunks retried; time to resume after failure.
            </li>
          </ul>
        </section>

        <section aria-labelledby="failures">
          <h2 id="failures">Failure modes → actions</h2>
          <ul>
            <li>
              <strong>Long snapshots block VACUUM/UNDO:</strong> shrink chunk
              size; run on a replica; pause between chunks.
            </li>
            <li>
              <strong>Log purged mid-snapshot:</strong> restart from last
              completed chunk; increase retention; consider replica.
            </li>
            <li>
              <strong>No primary key:</strong> define synthetic key or dedupe in
              sink (hash + latest timestamp).
            </li>
            <li>
              <strong>DDL during snapshot:</strong> retry chunk with stable
              schema; versioned views; schema-compat rules.
            </li>
            <li>
              <strong>Replica lag:</strong> validate boundary alignment; if
              laggy, snapshot from primary or temporarily gate OLTP.
            </li>
          </ul>
        </section>

        <section aria-labelledby="lab">
          <h2 id="lab">Hands-on: prove no gaps/dupes (≈15 min)</h2>
          <ol>
            <li>
              Pick your database and run the “boundary + consistent read”
              snippet.
            </li>
            <li>
              While snapshot runs, perform writes in a second session: insert
              (id=3), update (id=2).
            </li>
            <li>
              After handoff, insert (id=4). Verify sink via MERGE and run a dup
              check.
            </li>
          </ol>
          <details>
            <summary><strong>Dup check query</strong></summary>
            <pre><button class="copy-btn" type="button">Copy</button><code>SELECT id, COUNT(*) AS c
FROM dw.orders
GROUP BY id
HAVING COUNT(*) &gt; 1;</code></pre>
          </details>
        </section>

        <section aria-labelledby="gotchas">
          <h2 id="gotchas">Common gotchas</h2>
          <aside class="callout">
            <strong>Throughput vs safety:</strong> Aim for 5–30s chunk time;
            keep txn duration within retention.
          </aside>
          <aside class="callout">
            <strong>LOBs/wide rows:</strong> Exclude heavy columns from initial
            snapshot; backfill later.
          </aside>
          <aside class="callout">
            <strong>Partitioned tables:</strong> Snapshot partitions in
            deterministic order; beware partition switches.
          </aside>
          <aside class="callout">
            <strong>Time zones:</strong> Normalize to UTC to prevent
            duplicate-by-wall-clock.
          </aside>
        </section>
      </article>
</div>
