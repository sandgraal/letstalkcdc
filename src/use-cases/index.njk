---
layout: base.njk
title: "CDC Use Cases & Applications | CDC: The Missing Manual"
canonicalPath: "/use-cases/"
head_extra: |
  <meta
        content="Explore real-world CDC use cases, including real-time data warehousing, cache invalidation, auditing, microservices integration, and full-text search indexing."
        name="description"
      />
  <link rel="stylesheet" href="{{ '/assets/css/pages/use-cases.css' | url }}">

---
{% import "components/ui.njk" as ui %}

{{ ui.hero(heroConfig) | safe }}

      <article class="prose">
        <p>
          Beyond the theory, Change Data Capture enables a diverse set of
          powerful, real-time applications. Understanding these patterns is key
          to unlocking the strategic value of your data. Each use case
          demonstrates a shift from slow, periodic batch processing to a
          continuous, event-driven paradigm.
        </p>

        <section class="use-case-section" id="warehousing">
          <h3>Use Case 1: Analytics & Business Intelligence</h3>
          <h2>From Batch ETL to Real-Time ELT</h2>
          <p>
            <strong>The Problem:</strong> Business decisions based on stale,
            24-hour-old data are a competitive liability. Traditional nightly
            ETL jobs place a heavy, recurring load on production databases and
            deliver insights that are already out of date.
          </p>
          <p>
            <strong>The CDC Solution:</strong> Log-based CDC continuously
            streams every row-level change from operational (OLTP) databases.
            These events flow through a streaming platform like Kafka and are
            loaded into an analytical data warehouse (Snowflake, BigQuery,
            Redshift) in near real-time. The warehouse uses `MERGE` or `UPSERT`
            operations to efficiently apply these granular changes, keeping
            analytical tables perfectly synchronized with the source.
          </p>
          <div class="seq" role="group" aria-label="Warehousing flow">
            <div class="seq-head">Producer (OLTP/CDC)</div>
            <div class="seq-head">Streaming Platform</div>
            <div class="seq-head">Warehouse</div>
            <div class="seq-cell">CDC tails DB changes</div>
            <div class="seq-cell">Publish to Kafka topic(s)</div>
            <div class="seq-cell">Connector reads</div>
            <div class="seq-cell">—</div>
            <div class="seq-cell">—</div>
            <div class="seq-cell"><code>MERGE/UPSERT</code> into models</div>
          </div>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Accelerated Decision-Making:</strong> BI dashboards and
              reports reflect business operations up to the minute, not up to
              the day.
            </li>
            <li>
              <strong>Reduced Database Load:</strong> Eliminates the need for
              resource-intensive, full-table scans during nightly batch windows,
              improving source system performance.
            </li>
            <li>
              <strong>Fresher Data for ML:</strong> Machine learning models can
              be trained and served with more current data, improving prediction
              accuracy.
            </li>
          </ul>
        </section>

        <section class="use-case-section" id="microservices">
          <h3>Use Case 2: System Architecture</h3>
          <h2>Asynchronous Microservice Integration</h2>
          <p>
            <strong>The Problem:</strong> Services in a microservices
            architecture need to share data, but direct, synchronous API calls
            create tight coupling. If one service is down, it can cause a
            cascade of failures in the services that depend on it.
          </p>
          <p>
            <strong>The CDC Solution:</strong> CDC enables the
            <strong>Event-Carried State Transfer</strong> pattern via the
            Transactional Outbox. When a service makes a change (an `orders`
            service creates an order), it writes the business data and an event
            record to an "outbox" table in the same atomic transaction. The CDC
            agent streams only the committed outbox events to a Kafka topic.
            Downstream services (shipping`, `billing`) simply subscribe to this
            topic to receive guaranteed, in-order state changes without ever
            calling the `orders` service directly.
          </p>
          <div class="seq" role="group" aria-label="Outbox integration flow">
            <div class="seq-head">Producer (Service A)</div>
            <div class="seq-head">Kafka</div>
            <div class="seq-head">Consumers (B/C)</div>
            <div class="seq-cell">Write business row + outbox (same txn)</div>
            <div class="seq-cell">CDC publishes outbox events</div>
            <div class="seq-cell">Subscribe to topic</div>
            <div class="seq-cell">—</div>
            <div class="seq-cell">—</div>
            <div class="seq-cell">Idempotent upserts; no sync coupling</div>
          </div>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Increased Resilience:</strong> Services are decoupled. The
              `shipping` service can continue processing events even if the
              `orders` service is temporarily unavailable.
            </li>
            <li>
              <strong>Improved Scalability:</strong> Services can be scaled
              independently. You can add more `billing` consumers without
              affecting the `orders` service.
            </li>
            <li>
              <strong>Enhanced Service Autonomy:</strong> Each team can evolve
              its service and database schema without breaking downstream
              consumers, as long as the event contract is maintained.
            </li>
          </ul>
        </section>
        <section class="use-case-section" id="search-indexing">
          <h3>Use Case 5: Discovery & Search</h3>
          <h2>Near-Real-Time Full-Text Search Indexing</h2>
          <p>
            <strong>The Problem:</strong> Rebuilding a search index from scratch
            (hourly/daily) is wasteful and makes “search lag” a constant
            complaint.
          </p>
          <p>
            <strong>The CDC Solution:</strong> Stream row-level changes into a
            lightweight transformer that denormalizes documents and ships them
            to Elasticsearch/OpenSearch/Solr. <em>Upsert</em> on create/update;
            <em>delete</em> on tombstones. Use a deterministic
            <code>doc_id</code> to make replays idempotent.
          </p>
          <div class="seq" role="group" aria-label="Search indexing flow">
            <div class="seq-head">Source/CDC</div>
            <div class="seq-head">Doc Builder / Enricher</div>
            <div class="seq-head">Search Index</div>
            <div class="seq-cell">Row-level change events</div>
            <div class="seq-cell">Join/denormalize into documents</div>
            <div class="seq-cell">Upsert by <code>doc_id</code></div>
            <div class="seq-cell">Tombstones</div>
            <div class="seq-cell">Mark as delete</div>
            <div class="seq-cell">Delete from index</div>
          </div>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Fresh search:</strong> New/edited products, profiles, and
              content appear within seconds.
            </li>
            <li>
              <strong>Lower cost:</strong> No more full re-index jobs; only
              incremental updates.
            </li>
            <li>
              <strong>Better relevance:</strong> Can re-score or enrich on the
              fly during the doc build step.
            </li>
          </ul>
        </section>

        <section class="use-case-section" id="feature-stores">
          <h3>Use Case 6: Machine Learning</h3>
          <h2>Real-Time Features & Online/Offline Consistency</h2>
          <p>
            <strong>The Problem:</strong> Batch-built features drift from
            production state; models suffer from “training/serving skew.”
          </p>
          <p>
            <strong>The CDC Solution:</strong> Feed CDC events into a feature
            pipeline: write <em>offline</em> parquet/Delta/Iceberg tables for
            training and simultaneously upsert <em>online</em> features
            (Redis/DynamoDB/Spanner). Use the same <code>event_id</code>
            and schema contracts so replays are safe and the two stores stay
            consistent.
          </p>
          <div class="seq" role="group" aria-label="Feature store flow">
            <div class="seq-head">Stream Processor</div>
            <div class="seq-head">Offline (Lakehouse)</div>
            <div class="seq-head">Online Feature Store</div>
            <div class="seq-cell">Ingest CDC events</div>
            <div class="seq-cell">Write Parquet/Delta/Iceberg</div>
            <div class="seq-cell">
              Idempotent upserts by <code>entity_id</code>
            </div>
            <div class="seq-cell">Derive features</div>
            <div class="seq-cell">Training datasets</div>
            <div class="seq-cell">Serve features to models</div>
          </div>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Fresher predictions:</strong> models see current features,
              not yesterday’s.
            </li>
            <li>
              <strong>Consistency:</strong> identical events power training and
              serving paths.
            </li>
            <li>
              <strong>Faster iteration:</strong> add features with
              schema-checked rollouts.
            </li>
          </ul>
        </section>

        <section class="use-case-section" id="when-not-to-use">
          <h3>Guardrails</h3>
          <h2>When <em>Not</em> to Use CDC (or use with caution)</h2>
          <ul class="impact-list">
            <li>
              <strong>Ultra-high fan-out updates</strong> that require global
              ordering across entities (CDC provides per-key order only).
            </li>
            <li>
              <strong>Compute-heavy transformations</strong> that are cheaper in
              micro-batches; consider a streaming job with windowing instead of
              per-row fan-out.
            </li>
            <li>
              <strong>SaaS sources with unreliable webhooks</strong> without
              replay—buffer behind an Inbox/Outbox queue first.
            </li>
            <li>
              <strong>Hard deletes needed but source lacks tombstones</strong>
              (polling CDC) → add soft-delete markers or archival tables.
            </li>
          </ul>
        </section>
        <section class="use-case-section" id="auditing">
          <h3>Use Case 3: Security & Compliance</h3>
          <h2>Immutable Auditing and Data Lineage</h2>
          <p>
            <strong>The Problem:</strong> For regulatory compliance (SOX, GDPR,
            HIPAA) and security investigations, organizations need a complete,
            tamper-proof history of all changes made to critical data. Building
            this logic into every application is complex and error-prone.
          </p>
          <p>
            <strong>The CDC Solution:</strong> The stream of events produced by
            log-based CDC *is* a perfect, immutable audit log. Each event
            captures the "before" and "after" state of the data, a precise
            timestamp, the type of operation (`INSERT`, `UPDATE`, `DELETE`), and
            metadata about the transaction. This entire stream can be durably
            archived to low-cost object storage (like Amazon S3), creating a
            verifiable and replayable history of the data's entire lifecycle.
          </p>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Simplified Compliance:</strong> Easily prove data lineage
              and access patterns to auditors, satisfying strict regulatory
              requirements.
            </li>
            <li>
              <strong>Enhanced Security:</strong> Detect and investigate
              unauthorized or anomalous data changes by analyzing the raw event
              stream.
            </li>
            <li>
              <strong>Faster Debugging:</strong> "Replay" the event stream to
              understand how data reached a corrupted state, drastically
              reducing time-to-resolution for production incidents.
            </li>
          </ul>
        </section>

        <section class="use-case-section" id="cache-invalidation">
          <h3>Use Case 4: Application Performance</h3>
          <h2>Reliable Cache Invalidation</h2>
          <p>
            <strong>The Problem:</strong> Keeping an external cache (like Redis
            or Memcached) synchronized with the database is famously difficult.
            A common failure mode is serving stale data because the application
            logic failed to invalidate the cache after a database write.
          </p>
          <p>
            <strong>The CDC Solution:</strong> This pattern is elegant in its
            simplicity. A lightweight consumer service listens to the CDC stream
            from the primary database. When it receives an `UPDATE` or `DELETE`
            event for a specific record, it issues a corresponding `SET` or
            `DEL` command to the cache for that record's key. The responsibility
            for cache consistency is moved out of the critical application path
            into a simple, reliable, asynchronous process.
          </p>
          <p><strong>The Business Impact:</strong></p>
          <ul class="impact-list">
            <li>
              <strong>Improved User Experience:</strong> Guarantees that users
              are never shown stale data, improving trust and application
              quality.
            </li>
            <li>
              <strong>Simplified Application Code:</strong> Removes complex and
              error-prone cache management logic from the application layer.
            </li>
            <li>
              <strong>Better Performance:</strong> Allows applications to
              confidently and aggressively cache data, reducing load on the
              primary database and improving response times.
            </li>
          </ul>
        </section>
      </article>
