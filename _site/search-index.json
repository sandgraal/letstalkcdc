[
  {
    "path": "/",
    "title": "/",
    "text": "Learn why Change Data Capture (CDC) projects fail and how to build scalable, reliable, and production-ready data pipelines. A deep-dive series. Why Change Data Capture Still Breaks, and How To Get It Right. Build production-grade CDC pipelines, understand why they fail, and learn how to design resilient, real-time data architectures. Start the Journey Design System In the modern data landscape , the value of information is intrinsically linked to its timeliness. Decisions made on outdated data can lead to missed opportunities, inefficient operations, and a diminished competitive edge. This reality has exposed the limitations of traditional data integration methods like nightly batch jobs and paved the way for a more dynamic, real-time approach. This series shows you how to build production-grade CDC pipelines that deliver on the promise of real-time data. What is Change Data Capture (CDC)? Change Data Capture (CDC) is a set of software design patterns used to identify, capture, and deliver the changes made to data in a source system, most commonly a database. Instead of copying entire datasets at intervals, CDC focuses exclusively on incremental, row-level changes like INSERT, UPDA…"
  },
  {
    "path": "/overview/",
    "title": "Series Overview",
    "text": "Series hub for advanced CDC patterns. Exactly-once semantics, multi-tenancy, partitioning, and reconciliation. Series Overview Change Data Capture marks a fundamental evolution in data integration, moving away from the latent, resource-intensive world of batch processing and into the dynamic paradigm of real-time streaming. By capturing individual data changes as they occur, CDC provides a mechanism to keep disparate systems synchronized with minimal impact and sub-second latency. This technology is a strategic enabler, unlocking real-time analytics and forming the backbone of resilient, modern data architectures. Start with the fundamentals. Understand what Change Data Capture is and why it’s a cornerstone of modern data architecture through real-world use cases. Start Here Core Concept Interactive Introduction to CDC An interactive dashboard covering core concepts, methods, architectures, and the tooling ecosystem. Dive In! Core Concept Event Envelope Delivery Guarantees Keys vs payload, before/after images, tombstones; ALO vs EOS scope and per-key ordering. Dive In! Core Concept Materialization 101 (Upsert/Delete) Practical MERGE patterns for upserts deletes; compaction vs histo…"
  },
  {
    "path": "/strategy/",
    "title": "The Strategic Value of CDC | CDC: The Missing Manual",
    "text": "The Strategic Advantage of CDC Shift from slow, state-based data thinking to an event-driven culture. Learn how continuous change streams unlock new revenue, retire tech debt, and reduce operational risk. See the ROI Explore the Mindset Shift Change Data Capture is more than a technical pattern; it is a strategic enabler that translates directly into tangible business value. By shifting from periodic batch updates to a continuous stream of data changes, organizations can enhance decision-making, improve operational efficiency, and power modern digital initiatives. Business case: where ROI shows up Revenue lift: fresher recommendations, faster quoting, real-time inventory → higher conversion/attach. Cost reduction: retire nightly ETL, incremental loads instead of full rebuilds, fewer bespoke APIs. Risk & compliance: auditability of changes, faster fraud/abuse detection, consistent deletes via tombstones. Time-to-market: teams subscribe to streams without negotiating point-to-point integrations. A Philosophical Shift for Data Traditional data integration asks, \"What is the current state of the data?\" This state-oriented view leads to latent, point-in-time snapshots. CDC fundamentally…"
  },
  {
    "path": "/tooling/",
    "title": "CDC Tooling Comparison | CDC: The Missing Manual",
    "text": "The Modern CDC Toolkit Compare open-source, managed, and enterprise CDC platforms. Balance operational control, cost, and depth of guarantees before you pick your stack. Open Source Options Managed SaaS The market for Change Data Capture tools has matured significantly, offering a range of solutions that cater to different needs. Choosing the right tool involves balancing control, cost, and convenience. Open-Source Champions: Power and Flexibility Open-source tools are favored by organizations with strong in-house data engineering capabilities that require deep customization and want to avoid vendor lock-in. Debezium Debezium has emerged as the de facto open-source standard for log-based CDC. It is a distributed platform of connectors that runs on the Apache Kafka Connect framework, providing high-performance, low-latency connectors for a wide range of popular databases. Pros: Free to use (Apache 2.0 license), highly flexible, robust feature set, and backed by a large community. Cons: The primary challenge is operational complexity. It requires users to deploy, manage, scale, and monitor the underlying infrastructure, including Kafka and Kafka Connect, which requires significant te…"
  },
  {
    "path": "/errata/",
    "title": "CDC Nuances & Errata — Read Before You Ship",
    "text": "Corrections, caveats, and sharp edges in CDC systems. CDC Nuances Errata Bookmark the gotchas: delivery guarantees, snapshots, and backfills all hide sharp edges. Use this checklist before declaring “exactly once.” Delivery Caveats Snapshot + Replay Notes A living list of precise clarifications, corrections, and “gotchas”. Treat this as pre‑flight checks before promising exactly‑once or “zero data loss.” End‑to‑end exactly‑once vs. effectively‑once Source connectors are at‑least‑once in practice. You can achieve effectively‑once by making sinks idempotent (UPSERT on stable keys) and/or deduplicating using a durable event_id ledger. Transaction boundaries : Database transactions map to change streams, not to sink transactions unless your pipeline coordinates them. Don’t assume cross‑topic atomicity. Per‑key ordering only : Ordering is guaranteed per partition key (primary/business key). Cross‑entity ordering is not guaranteed and shouldn’t be relied upon. Snapshots, backfills, and replays Initial snapshots can interleave with live changes; design consumers to reconcile using version columns or op_ts . Incremental snapshots (signal‑based) are powerful but can produce duplicates; alwa…"
  },
  {
    "path": "/event-envelope/",
    "title": "Event Envelope & Delivery Guarantees",
    "text": "Break down the structure of CDC change events and learn how at-least-once or exactly-once delivery affects consumers. Design the Event Envelope Learn how CDC tools package change events, why before/after images matter, and how delivery guarantees shape your downstream processing. See Event Anatomy Lock Schema Contract Readiness tracker Event anatomy Before/after images Delivery guarantees Ordering implications Schema contracts Connector crosswalk Playbooks Validation Readiness scorecard Monitoring guardrails Further resources Envelope readiness tracker Roll up your readiness checklist progress before you dive into the deep-dive sections. 0 of 0 readiness checks complete (0%) No readiness checklists available yet. Reset all envelope checklists What lives inside the envelope Change events are more than a JSON blob. Every field exists to keep data, ordering, and metadata aligned across services. Ground your consumers in the same vocabulary so investigations stay fast. { \"op\": \"u\", \"ts_ms\": 1712857612456, \"source\": { \"db\": \"app\", \"table\": \"orders\", \"lsn\": 368592330 }, \"transaction\": { \"id\": \"4.25.901\", \"total_order\": 442, \"event_count\": 2 }, \"before\": { \"order_id\": 42, \"status\": \"proce…"
  },
  {
    "path": "/materialization/",
    "title": "Materialization 101",
    "text": "Translate CDC change events into reliable warehouse tables with upsert, delete, and history-friendly patterns. Materialization 101 Master the merge logic that turns change events into durable, queryable tables across warehouses, lakes, and caches. Compare Patterns Ship Quality Gates Readiness tracker Materialization patterns Idempotent merge logic SQL templates Maintaining history Late arrivals Operational guardrails Data quality checks Verification playbooks Readiness scorecard Further resources Materialization readiness tracker Track how close each readiness checklist is to production-ready materialization in a single view. 0 of 0 readiness checks complete (0%) No readiness checklists available yet. Reset all materialization checklists Choose the right pattern for the job Materialization strategies compared Pattern How it works Best for Mutable table (upsert/delete) Apply change events directly to a target table using merge logic. Serving layer caches, feature stores, operational analytics. Streaming merge Continuous ingestion job (dbt incremental, Snowpipe, Flink) performs row-level MERGE as events arrive. Low-latency marts where freshness is minutes, not hours. Append-only with…"
  },
  {
    "path": "/observability/",
    "title": "Observability Basics",
    "text": "Monitor CDC connectors with golden signals, actionable dashboards, and alerting policies that keep downstream consumers safe. Observability Basics Instrument your CDC platform with lag, throughput, error, and saturation signals so incidents are caught before consumers notice. Build Dashboards Define SLOs Readiness tracker Golden signals Dashboards Alert policy SLOs Tracing logs Runbook hygiene Post-incident reviews Readiness scorecard Further resources Observability readiness tracker See overall monitoring maturity at a glance before diving into each checklist. 0 of 0 readiness checks complete (0%) No readiness checklists available yet. Reset all observability checklists Golden signals for CDC Borrow the SRE playbook—latency, traffic, errors, and saturation—but map each to a CDC-specific metric. Lag your north star Signal CDC metric Why it matters Latency End-to-end lag (source commit to sink ingest). Shows whether consumers see fresh data. Traffic Events per second per connector. Highlights surges or drops in change volume. Errors Failed batches / DLQ counts. Detects poison pills or downstream outages. Saturation Connector CPU, thread pool usage, or sink credits consumed. Warns yo…"
  },
  {
    "path": "/ops-offsets/",
    "title": "Ops: Offsets & Replays",
    "text": "Operate CDC pipelines with confidence by hardening offset storage, planning safe rewinds, and scripting replay drills. Ops Playbook: Offsets Replays Keep change streams healthy by protecting offset stores, practicing safe rewinds, and rehearsing replay drills. View Ops Checklist Automate Replays Readiness tracker Ops checklist Offset storage patterns Safe rewind procedure Replay drills Monitoring Automation Quarterly audits Readiness scorecard Further resources Offset operations readiness tracker Watch overall rewind preparedness climb as you complete each operational checklist. 0 of 0 readiness checks complete (0%) No readiness checklists available yet. Reset all offset checklists Guardrail checklist Offsets persist to durable storage separate from the connector host. Backups exist for the last seven days of offset snapshots. Runbooks document how to pause, rewind, and resume each connector. DLQ triage flow is rehearsed quarterly. Access to offset stores is restricted to on-call engineers (MFA enforced). Offset storage patterns Treat offsets as crown jewels. They decide whether a replay is a clean rewind or a week-long dedupe exercise. Choose durable storage intentionally Pipeline…"
  },
  {
    "path": "/from-change-capture-to-ci/",
    "title": "Let’s Talk CDC #7 — From Change Capture to Continuous Integration",
    "text": "How treating data pipelines like code bridges the gap between CDC and CI/CD. Let’s Talk CDC #7 — From Change Capture to Continuous Integration How treating data pipelines like code bridges the gap between CDC and CI/CD. Change Data Capture (CDC) has long been treated as plumbing—low-level extraction with little alignment to software delivery. In this talk, we explore what happens when CDC joins the CI/CD lifecycle : Version-controlled configurations for agents and connectors Stream-aware automated testing Blue-green promotion for data pipelines GitHub Actions + Matillion orchestration examples The goal: streaming pipelines as deployable artifacts . By the end, you’ll see how DevOps thinking closes the loop between change, delivery, and data integrity."
  },
  {
    "path": "/404.html",
    "title": "404",
    "text": "Page not found — CDC: The Missing Manual We couldn’t find that page the link may be outdated or mistyped. here are quick paths to popular sections. ← there's no place like home series overview"
  },
  {
    "path": "/connector-builder/",
    "title": "Connector Config Builder — Debezium (Postgres • MySQL • Oracle)",
    "text": "Generate Debezium connector configs with the right settings for DLQ, snapshot modes, and filtering. Connector Config Builder Generate Debezium connector configs for Postgres, MySQL, or Oracle. Pick your version, set filters and DLQ options, then copy or curl straight to Kafka Connect. Choose a Source Preview Copy ← Back to Overview 1) choose source version postgres mysql oracle debezium version: 2.x (topic.prefix) 1.x (database.server.name) 2) connection naming connector name: connect url: database: username: password: dlq topic (optional): 3) filters snapshot table filtering include regex: exclude regex: snapshot options snapshot mode: initial initial_only never fetch size: advanced settings leave defaults alone unless you know the operational impact. overrides are preserved below. tasks.max: heartbeat interval ms: 4) preview apply copy curl to Kafka Connect copy Remember: apply config changes with PUT /connectors/ name /config to avoid task bounce."
  },
  {
    "path": "/debezium-decoder/",
    "title": "Debezium Event Decoder",
    "text": "Paste Kafka records to inspect metadata, view before/after diffs, and generate MERGE templates — all in-browser. Debezium Event Decoder Paste Kafka records to inspect op/source metadata, compare before vs after payloads, and generate ready-to-tweak MERGE templates. Runs entirely in your browser. Paste Events Review Output ← Back to Overview 1) paste event(s) paste lines from kafka-console-consumer (schema-less JSON) or Connect DLQ records. multiple lines ok; non-JSON lines are ignored. try sample flatten nested fields (dot-notation) privacy: nothing leaves this page. 2) keys sink key columns: suggest comma-separate multi-column keys ( acct_id,order_id ). postgres snowflake bigquery databricks delta mysql redshift copy SQL -- paste an event to generate a MERGE/UPSERT template… we infer columns from your last event’s after (or before for deletes). adjust names as needed. decoded"
  },
  {
    "path": "/design-system/",
    "title": "Design System",
    "text": "Tokens, components, and interaction guidelines for Let’s Talk CDC. Design Language CDC Manual Design System Shared tokens and interaction patterns power a consistent, accessible experience across the CDC learning series. Use this page as the source of truth when building new pages, illustrations, or interactive labs. Foundations Core decisions for typography, color, and spacing. Tokens live in src/assets/css/styles.css and cascade across light and dark themes. Accent / Primary var(--accent-primary) Accent / Hover var(--accent-hover) Surface / Base var(--bg-primary) Surface / Secondary var(--bg-secondary) Text / Primary var(--text-primary) Text / Secondary var(--text-secondary) Border var(--border-color) Success var(--success-color) Warning var(--warning-color) Danger var(--danger-color) Reliable CDC pipelines begin with clarity. Display · var(--type-display) Bring change data capture to production. Hero · var(--type-hero) Exactly-once semantics Title · var(--type-title) Snapshot checklists Heading · var(--type-heading) CDC enables real-time data movement with minimal source impact. Body · var(--type-body) Used for meta text, captions, and data labels. Small · var(--type-sm) Spacing…"
  },
  {
    "path": "/dlq-triage/",
    "title": "DLQ Triage Assistant — find, explain, fix",
    "text": "Guided DLQ troubleshooting for Kafka Connect/Debezium: list DLQ messages, inspect headers, decode payloads, and map errors to fixes. DLQ Triage Assistant Point this at your DLQ topic and connector name. Generate safe commands to inspect headers, unwrap payloads, and map errors to fixes. Back to Overview Start Debugging ← Back to Overview 1) inputs bootstrap: dlq topic: connector name: consumer group (sink): 2) enumerate peek copy # count a sample show latest offsets kafka-run-class kafka.tools.GetOffsetShell \\ --broker-list localhost:29092 --topic dlq.inventory --time -1 # peek a few DLQ records (value only) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --timeout-ms 4000 \\ --max-messages 5 | jq -C . show headers decode original value copy # show headers (kafka cli shows headers only with Kafkacat or kcat) # try kcat if available (brew install kcat) kcat -b localhost:29092 -t dlq.inventory -C -J -c 5 | jq -C . # extract original payload if record is wrapped (common in Connect DLQs) kafka-console-consumer --bootstrap-server localhost:29092 \\ --topic dlq.inventory --from-beginning --timeout-ms 4000 --max-messages 5 \\ | jq -r '.origi…"
  },
  {
    "path": "/exactly-once/",
    "title": "Exactly-Once Semantics",
    "text": "A deep dive into at-least-once vs. exactly-once semantics, idempotency, and the transactional outbox pattern for reliable data pipelines. Exactly-Once Processing, For Real Cut through marketing myths and learn how to deliver CDC pipelines that behave idempotently under failure, even when end-to-end EOS is impossible. See the Failure Mode Run the Interactive Demo End-to-end \"exactly-once delivery \" across heterogeneous systems (DB → Kafka → warehouse/search) is not achievable in general. What we actually ship is Exactly-Once Processing (EOP) : combine At-Least-Once (ALO) transport with idempotent sinks so replays don’t create duplicates. Scope matters: Source connectors (e.g., Debezium): ALO into Kafka. They can’t make the external database “transactional” with Kafka. Within Kafka→Kafka topologies: EOS is possible using Kafka transactions (idempotent producers + transactional offsets), but the scope is limited to Kafka topics. Kafka → external sinks: Effective EOS comes from idempotent upserts (or staging+MERGE) at the destination, not from transport guarantees alone. How a CDC pipeline behaves during failures is defined by its data delivery guarantees: At-Most-Once: The weakest gua…"
  },
  {
    "path": "/intro/",
    "title": "Interactive Introduction to CDC",
    "text": "An interactive dashboard covering core CDC concepts, methods, architectures, and the tooling ecosystem. Interactive Introduction to CDC Start here if you're moving from batch ETL to real-time pipelines. You'll get the foundations, architectural trade-offs, and tooling map you need before diving into the advanced playbooks. Jump to Methods Compare Approaches View Learning Path Start here Methods Architecture Outbox Operations Tooling Next steps Change Data Capture ( CDC ) is the discipline of replicating data changes from a source database to downstream systems in near real time—without heavy full refreshes. Outcome You leave with a mental model for CDC, the vocabulary to compare approaches, and a roadmap into the deeper modules. Who it’s for Data engineers modernizing nightly batch ETL into streaming pipelines. Platform and infra teams tasked with selecting CDC tooling. Architects who need common language across producers, brokers, and sinks. For decades, moving data between systems meant relying on slow, resource-intensive batch jobs that would run overnight. This approach is no longer viable in a world that demands real-time data. How do you keep disparate systems synchronized in…"
  },
  {
    "path": "/lab-kafka-debezium/",
    "title": "Hands-On Lab: CDC with Kafka, Debezium, and Postgres | CDC: The Missing Manual",
    "text": "A step-by-step guide to building a real-time Change Data Capture pipeline using Docker, Postgres, Kafka, and Debezium. Hands-On Lab: Kafka + Debezium + Postgres Spin up a fully working CDC stack with Docker Compose, configure Debezium, and watch real-time changes flow end to end. No prior streaming ops experience required. Check Prerequisites Start Building In this lab, you'll build a complete, end-to-end Change Data Capture (CDC) pipeline from scratch. We'll use Docker Compose to orchestrate a Postgres database, Kafka, and the Debezium Postgres connector to stream row-level changes in real time. Prerequisites Docker and Docker Compose: Ensure they are installed and running on your machine. A terminal or command prompt: Basic familiarity with shell commands. cURL or a similar tool: For interacting with the Kafka Connect REST API. Lab Setup Create Project Directory Start by creating a new folder for your lab project and navigate into it. mkdir cdc-lab && cd cdc-lab Create `docker-compose.yml` Create a file named docker-compose.yml and paste the following configuration. This file defines all the services we need: Zookeeper, Kafka, Postgres, and Debezium/Connect. # docker-compose.yml …"
  },
  {
    "path": "/merge-cookbook/",
    "title": "merge / upsert cookbook — sinks for CDC",
    "text": "shared assumptions (adapt to your schema) staging table: STG_CUSTOMERS (raw CDC) with columns ID , EMAIL , OP (c/u/d), OP_TS (event time), optional VERSION or LSN/GTID/SCN . target table: TARGET_CUSTOMERS (latest state). business key: ID . replace with your key columns as needed. late events: we accept an incoming row if its OP_TS ≥ current target OP_TS . soft deletes: either actually delete or set a IS_DELETED flag. if your source doesn’t emit OP_TS , use commit LSN/GTID/SCN as the ordering surrogate. snowflake MERGE INTO TARGET_CUSTOMERS t USING ( SELECT ID, EMAIL, OP, OP_TS FROM STG_CUSTOMERS QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) = 1 ) s ON t.ID = s.ID WHEN MATCHED AND s.OP = 'd' AND s.OP_TS = t.OP_TS THEN DELETE WHEN MATCHED AND s.OP IN ('c','u') AND s.OP_TS = t.OP_TS THEN UPDATE SET EMAIL = s.EMAIL, OP_TS = s.OP_TS WHEN NOT MATCHED AND s.OP 'd' THEN INSERT (ID, EMAIL, OP_TS) VALUES (s.ID, s.EMAIL, s.OP_TS); dedup in the USING subquery protects against duplicate events in staging. bigquery MERGE `dataset.TARGET_CUSTOMERS` t USING ( SELECT ID, EMAIL, OP, OP_TS FROM `dataset.STG_CUSTOMERS` QUALIFY ROW_NUMBER() OVER (PARTITION BY ID ORDER BY OP_TS DESC) =…"
  },
  {
    "path": "/mermaid-sandbox/",
    "title": "mermaid-sandbox",
    "text": "Mermaid Sandbox html, body { height: 100%; } body { margin: 0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; background: #0b1220; color: #e6eefc; } .wrap { padding: 12px; } .error { color: #fca5a5; white-space: pre-wrap; } Skip to content sequenceDiagram autonumber participant App as App Service participant DB as OLTP DB participant OB as Outbox Table participant Bus as Event Bus participant Sink as Consumer App- DB: BEGIN TX App- DB: UPDATE domain rows App- OB: INSERT event record DB-- App: COMMIT OB-- Bus: Outbox relay publishes Bus-- Sink: Consume and apply (idempotent)"
  },
  {
    "path": "/multi-tenancy/",
    "title": "Multi-Tenancy — Isolation vs Cost | CDC: The Missing Manual",
    "text": "Interactive explorer for multi-tenant CDC: isolation levels, topic strategy, consumer groups, and egress sizing with shareable presets. Multi-Tenancy: Cost vs Isolation Model isolation levels, topic strategies, and egress math for shared CDC platforms. See how per-tenant choices affect blast radius, spend, and operations. Start with the Primer Tweak Isolation Controls Primer: Why multi-tenancy matters for CDC Sharing infrastructure saves money but couples tenants together. More isolation (per-tenant topics or clusters) reduces blast radius and gives clearer SLAs, but increases cost and operational load. Isolation ladder: shared topics → per-tenant topics → per-tenant clusters. Egress (external) ≈ tenants × change_rate × (payload + envelope + overhead) . Broker egress adds replication factor: intra_broker ≈ external × (RF − 1) . Storage at retention ≈ egress_bytes_per_s × (86400 × days) ÷ compression . Operational heat: total partitions = topics × partitions_per_topic (impacts rebalances, ISR, & quotas). RBAC/retention easier per-tenant topics; cluster isolation for regulators/VIPs. Guardrails: per-tenant produce/consume quotas, DLQ policy, retention/compaction per topic, and ACLs s…"
  },
  {
    "path": "/oracle-notes/",
    "title": "Vendor Nuances — Oracle CDC | CDC: The Missing Manual",
    "text": "Oracle Change Data Capture: prerequisites, quick checks, performance knobs, and troubleshooting playbook for log-based CDC (LogMiner/replication readers). ← Back to Quickstarts Vendor Nuances — Oracle (Log-Based CDC) Practical prerequisites, quick checks, performance knobs, and a “first-aid” section for common failures. What you must have before turning on CDC Database in ARCHIVELOG mode (recommended for CDC resiliency). Supplemental logging enabled (database-level minimal; table-level as needed for keys/columns used in merges). Privileges : read data dictionary views, redo/archive logs, target tables; ability to create log groups if required. Redo retention sized for snapshot + catch-up window (avoid log switch storms during backfills). Stable primary keys (or explicit “replica identity full” equivalents via table log groups for keyless tables). Quick Health Checks (copy/paste) Archivelog & Redo Status SELECT log_mode FROM v$database; SELECT sequence#, bytes/1024/1024 MB, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 10 ROWS ONLY; Supplemental Logging -- database-wide SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; -- table-level (co…"
  },
  {
    "path": "/partitioning/",
    "title": "Partitioning & Reconciliation — Keys, Skew, Late Arrivals | CDC: The Missing Manual",
    "text": "Learn partitioning and reconciliation for CDC: choosing keys, handling skew, preserving per-key order, and making late arrivals safe with version and watermarks. Partitioning Reconciliation Design Kafka topics that preserve per-entity ordering, tame skew, and make late arrivals safe with watermarks and versioning. Respect Ordering Pick Partition Keys The Golden Rule of Kafka Ordering Kafka guarantees the order of messages within a single partition . It makes no ordering guarantees across different partitions. Therefore, to maintain the correct sequence of changes for any given entity (like a specific customer or a single product), all changes for that entity must be sent to the same partition. This is the fundamental purpose of a partition key. Partition sizing (rule-of-thumb) Start with a target throughput per partition your consumers can sustain (1–5k events/s). Baseline: partitions ≈ ceil(total_events_per_s ÷ target_per_partition) Include skew headroom: multiply by 1.5–2× if distribution is Zipf-y or you have hot tenants. Changing partitions later: adding partitions does not rehash existing keys; plan key strategy up front or use topic splits. How to Choose a Good Partition Key …"
  },
  {
    "path": "/quickstarts/quickstart-mssql/",
    "title": "MS SQL Server CDC Quickstart | CDC: The Missing Manual",
    "text": "MS SQL Server CDC quickstart: enable CDC, configure SQL Server Agent, set up a connector, verify changes, and roll back safely. MS SQL Server CDC — Quickstart Runbook Enable database-level CDC, configure the SQL Server Agent, register a connector, and verify change data flow. Back to Quickstarts Prerequisites (DB) SQL Server Agent must be running. Database should use Full Recovery Mode . User needs the db_owner fixed database role to enable CDC. Tables require a stable primary key or unique index. Provision disk for transaction log and CDC change tables. -- Is SQL Server Agent running? EXEC master.dbo.xp_servicecontrol 'QueryState', 'SQLServerAGENT'; -- Recovery model SELECT name, recovery_model_desc FROM sys.databases WHERE name = 'YourDB'; DB Setup (copy/paste) -- Enable CDC on the database USE YourDB; GO EXEC sys.sp_cdc_enable_db; GO -- Enable CDC on a specific table EXEC sys.sp_cdc_enable_table @source_schema = N'dbo', @source_name = N'customers', @role_name = N'cdc_role', @supports_net_changes = 1; GO -- Grant access to the CDC user CREATE USER cdc WITHOUT LOGIN; GRANT SELECT ON ALL cdc.change_tables TO cdc; Connector Config (example) { \"name\": \"mssql-cdc\", \"config\": { \"connec…"
  },
  {
    "path": "/quickstarts/quickstart-mysql/",
    "title": "MySQL CDC Quickstart | CDC: The Missing Manual",
    "text": "MySQL CDC quickstart: prerequisites, connector config, verification, acceptance tests, and rollback. MySQL CDC — Quickstart Runbook Configure binlog safely, register a connector, verify changes, and roll back cleanly. Back to Quickstarts Prerequisites (DB) binlog_format=ROW with binlog_row_image FULL or MINIMAL per connector support. GTID mode preferred ( gtid_mode=ON ). Binlog retention window ≥ snapshot + catch-up time. Stable primary keys (or sink logic that can reconcile keyless tables). binlog_format=ROW captures per-row changes for high-fidelity CDC. binlog_row_image controls how much data is logged, and gtid_mode=ON simplifies failover by uniquely identifying each transaction across replicas. SHOW VARIABLES LIKE 'binlog_format'; SHOW VARIABLES LIKE 'binlog_row_image'; SHOW VARIABLES LIKE 'gtid_mode'; SHOW MASTER STATUS; SHOW VARIABLES LIKE 'binlog_expire_logs_seconds'; DB Setup (copy/paste) # my.cnf (server) binlog_format = ROW binlog_row_image = FULL server_id = 1 gtid_mode = ON enforce_gtid_consistency = ON # grant minimal privileges to CDC user GRANT REPLICATION SLAVE, REPLICATION CLIENT, SELECT ON *.* TO 'cdc'@'%' IDENTIFIED BY 'cdc'; Connector Config (example) { \"name\":…"
  },
  {
    "path": "/quickstarts/quickstart-oracle/",
    "title": "Oracle CDC Quickstart | CDC: The Missing Manual",
    "text": "Oracle CDC quickstart: prerequisites (ARCHIVELOG, supplemental logging), health checks, connector notes, verification, and rollback. Oracle CDC — Quickstart Runbook Confirm prerequisites, collect health signals, register a connector, verify changes, and roll back safely. Back to Quickstarts Prerequisites (DB) ARCHIVELOG mode recommended. Supplemental logging enabled (DB-level minimal; table-level for key columns or ALL for keyless tables). Redo/archive retention window ≥ snapshot + catch-up time. Stable primary keys (or table log groups capturing merge keys). ARCHIVELOG mode keeps redo history available so CDC readers can recover. Supplemental logging ensures redo contains key columns to reconstruct before/after images, which is critical when applying updates downstream. -- archivelog and redo SELECT log_mode FROM v$database; SELECT sequence#, archived, status FROM v$log ORDER BY first_time DESC FETCH FIRST 5 ROWS ONLY; -- supplemental logging SELECT supplemental_log_data_min, supplemental_log_data_all FROM v$database; SELECT owner, table_name, log_group_name, always, log_group_type FROM dba_log_groups ORDER BY owner, table_name; DB Setup (examples) -- table-level log group for key…"
  },
  {
    "path": "/quickstarts/quickstart-postgres/",
    "title": "Postgres CDC Quickstart | CDC: The Missing Manual",
    "text": "Postgres CDC quickstart: prerequisites, connector config, verification, acceptance tests, and safe rollback. Postgres CDC — Quickstart Runbook Set the right DB knobs, register a connector, verify changes, and know how to roll back safely. Back to Quickstarts Prerequisites (DB) wal_level=logical , slots allowed, publication permissions. This enables logical decoding so Debezium can stream row-level changes from the transaction log. It also keeps logical replication slots consistent for downstream consumers. Tables have a stable primary key (or configure REPLICA IDENTITY FULL for keyless tables). WAL retention window ≥ snapshot + catch-up time. SHOW wal_level; SHOW max_replication_slots; SHOW max_wal_senders; SELECT current_setting('wal_keep_size'); DB Setup (copy/paste) -- minimal role for logical replication (adjust as needed) CREATE ROLE cdc LOGIN PASSWORD 'cdc' REPLICATION; -- ensure replica identity for keyless tables (example) ALTER TABLE app.customer REPLICA IDENTITY FULL; -- create publication if you prefer manual management CREATE PUBLICATION cdc_pub FOR TABLE app.customer; Debezium can auto-create a filtered publication; configure manually if you need tighter scope. Connect…"
  },
  {
    "path": "/quickstarts/",
    "title": "CDC Quickstart Guides",
    "text": "Hands-on, practical Change Data Capture quickstart guides for Postgres, MySQL, and Oracle. CDC Quickstart Guides Bring your first change data capture pipeline online fast. Choose your source database and follow a copy-paste friendly walkthrough with checkpoints and sanity checks. Pick a Database Run Acceptance Tests Start with Postgres Postgres Configure logical replication, set up a Debezium connector, and verify real-time change events. MySQL Enable binlog, configure a Debezium connector, and stream row-level changes to a Kafka topic. Oracle Set up supplemental logging and leverage LogMiner to capture change events in real time. MS SQL Server Enable database-level CDC, configure SQL Server Agent, and validate change data flow end-to-end. Quickstart cheat sheet Prep each environment fast with prerequisites, time estimates, and direct links to the walkthroughs. 🐘 Postgres ~10–15 min Enable logical decoding, register Debezium, verify change events end-to-end. wal_level=logical , slots allowed Primary keys or REPLICA IDENTITY FULL Open Postgres quickstart Run Acceptance Tests 🐬 MySQL ~10–15 min Put the binlog in ROW mode, narrow includes, confirm Debezium captures changes. binlog_f…"
  },
  {
    "path": "/schema-evolution/",
    "title": "Schema Evolution | CDC: The Missing Manual",
    "text": "Handling Schema Evolution in CDC Keep downstream systems humming as source schemas change. Learn how schema registries, compatibility modes, and message contracts safeguard your pipelines from drift. Use a Schema Registry Choose Compatibility Modes Source database schemas are not static. Over the lifecycle of an application, developers will inevitably make changes: adding new columns, removing old ones, or modifying data types. This phenomenon, known as schema drift or schema evolution , is a primary cause of brittle data pipelines. A pipeline not designed for this will break the moment an incoming change event no longer matches what downstream consumers expect. The Solution: The Schema Registry (An API Contract for Your Data) The industry-standard solution is to treat your data's schema like a formal API contract. This is managed by a Schema Registry . Think of it as the 'OpenAPI' or 'Swagger' for your event streams. It's a centralized service that ensures all data producers and consumers agree on the 'shape' of the data, even as that shape evolves over time. By using a schema-aware format like Apache Avro or Protobuf, producers and consumers can be decoupled, allowing them to evo…"
  },
  {
    "path": "/snapshotting/",
    "title": "Snapshotting (Initial Load)",
    "text": "Learn how to handle the initial data load (snapshotting) in a CDC pipeline consistently and without data loss. The First Hurdle: Snapshotting Initial loads are where most CDC rollouts stumble. Follow this guided playbook to capture a consistent snapshot, hold the boundary, and merge the stream without gaps. Start the Playbook Skip to DB Recipes Playbook overview Checklist Concept DB recipes Start with a clean handoff 80% of CDC escalations originate during the initial load. Teams rush the boundary, purge logs, and spend weeks replaying tables. Use these three moves to align engineering, operations, and analytics before the stream goes live. Prove it in 15 minutes → High-watermark boundary handoff 1 Mark high-watermark Record LSN/SCN boundary → 2 Consistent read Snapshot without gaps → 3 Stream after boundary Apply idempotently Snapshot → high-watermark → stream. Preserve per-key order and use idempotent upserts/deletes at the sink. ① Boundary & retention. Set the log high-watermark, confirm retention covers the run, and name an owner for recovery points. ② Execution made safe. Pick the right snapshot mode, chunk large tables, and cap parallelism to respect OLTP workloads. ③ Proof &…"
  },
  {
    "path": "/tests/",
    "title": "Acceptance Tests — Kafka + Debezium Lab | CDC: The Missing Manual",
    "text": "Run acceptance tests for the Kafka + Debezium lab: stack up, connector healthy, topic has change events, and (optional) chaos smoke. Acceptance Tests — Kafka + Debezium Lab Run copy/paste scripts on your machine to prove the lab stack is healthy: containers up, connector RUNNING, topics streaming events, and restarts staying idempotent. Back to Quickstarts What You’ll Run scripts/test_stack.sh — Zookeeper, Broker, Connect, PG are running & Connect REST is reachable scripts/test_connector.sh — Connector exists and is RUNNING scripts/test_events.sh — Topic has data; consumed messages parse as JSON and include op ∈ {c,u,d} scripts/test_chaos_smoke.sh (optional) — Latest offsets increase after a controlled connector restart Requirements: docker , docker compose , curl , jq . Windows users: run under WSL or Git Bash. Run Them (Mac/Linux + WSL) chmod +x scripts/*.sh bash scripts/test_stack.sh bash scripts/test_connector.sh bash scripts/test_events.sh # optional: bash scripts/test_chaos_smoke.sh Expect green checks (✅). Any ❌ includes the failing step so you know what to fix. Windows (PowerShell via WSL) Open “Ubuntu (WSL)” cd into the lab folder you mounted ( /mnt/c/Users/you/lab ) Run t…"
  },
  {
    "path": "/troubleshooting/",
    "title": "First 15 Minutes — CDC Troubleshooting",
    "text": "First 15 Minutes: CDC Troubleshooting A repeatable triage to stabilize incidents fast. Keep it pragmatic, measurable, and reversible. Quick Triage (5–8 minutes) Freeze changes that make state drift: pause backfills, schema changes, and connector restarts. Define the failure mode : stuck (no progress), slow (lag growing), wrong (duplicates, missing rows), or crashing . Bound the blast radius : single table/tenant vs global; snapshot vs stream. Checkpoint evidence (timestamps, connector name, offsets, group id). Don’t tail logs without writing down the top lines you see. Choose a safety : if logs are at risk of rotating out, extend retention (WAL/binlog/redo) before touching the pipeline. Goal: stop data loss, capture proof, buy time. Detailed checks below. Artifacts to Collect (Copy/Paste) Versions: source db, connector (Debezium), broker (Kafka/Redpanda), sink Connector config (sanitized): include snapshot mode, includes/excludes, heartbeat Exact error lines: 20–50 lines around the first failure Lag/offsets: consumer group lag and last committed LSN/GTID/SCN Log retention settings: WAL/binlog/redo retention and current oldest log Commands Kafka (Consumer Lag) copy kafka-consumer-gr…"
  },
  {
    "path": "/use-cases/",
    "title": "CDC Use Cases & Applications | CDC: The Missing Manual",
    "text": "CDC in the Wild Explore the real-world patterns powered by change streams—from real-time analytics to cache invalidation and microservice choreography. Analytics BI Microservices Integration Beyond the theory, Change Data Capture enables a diverse set of powerful, real-time applications. Understanding these patterns is key to unlocking the strategic value of your data. Each use case demonstrates a shift from slow, periodic batch processing to a continuous, event-driven paradigm. Use Case 1: Analytics & Business Intelligence From Batch ETL to Real-Time ELT The Problem: Business decisions based on stale, 24-hour-old data are a competitive liability. Traditional nightly ETL jobs place a heavy, recurring load on production databases and deliver insights that are already out of date. The CDC Solution: Log-based CDC continuously streams every row-level change from operational (OLTP) databases. These events flow through a streaming platform like Kafka and are loaded into an analytical data warehouse (Snowflake, BigQuery, Redshift) in near real-time. The warehouse uses `MERGE` or `UPSERT` operations to efficiently apply these granular changes, keeping analytical tables perfectly synchroniz…"
  }
]